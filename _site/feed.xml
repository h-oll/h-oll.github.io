<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-06-09T22:46:56+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Harold Ollivier</title><subtitle>Quantum Information Processing Researcher in the QAT team -- Computer Science Department, Ecole Normale Supérieure</subtitle><entry><title type="html">Comments on the Quantum Software Manifesto</title><link href="http://localhost:4000/2025/06/09/quantum_software_manifesto.html" rel="alternate" type="text/html" title="Comments on the Quantum Software Manifesto" /><published>2025-06-09T00:00:00+02:00</published><updated>2025-06-09T00:00:00+02:00</updated><id>http://localhost:4000/2025/06/09/quantum_software_manifesto</id><content type="html" xml:base="http://localhost:4000/2025/06/09/quantum_software_manifesto.html"><![CDATA[<p>EQSI has published the <a href="https://www.eqsi.org/about/quantum-software-manifesto/">Quantum Software
Manifesto</a>. It
aims to reach out to policy makers and the wider public to emphasize
the role of software in the development of quantum information
processing capabilities.</p>

<p>It does a good job at showing that the scientific community working on
quantum software is well aware of the associate challenges and tries
to address them in a way that is most beneficial for the economic
development of the field.</p>

<p>While I’m very happy that such initiative exists, I think parts could
be improved an dclarified. Here is a list of my main comments:</p>

<h1 id="comments-on-introduction">Comments on “Introduction”</h1>

<p><strong>“Quantum computers have the potential to solve important problems much faster”</strong></p>

<p>I would not say much faster, but much more efficiently. This is more
accurate in my opinion and also could possibly point toward advantages
other than speed — think about precision or energy.</p>

<p><strong>“quantum software”</strong></p>

<p>Indeed, but we could also point out that it needs both quantum and
classical software. And maybe we could mention a renewedd algorithmics
effort to create this software. In a sense, software might not reflect
fully what we need, wich is quite beyond creating a piece of software
like you would create a website. We need more quantum and classical
algorithms and turn them into software afterwards.</p>

<p>Writing this, I’m getting convinced that this is quite crucial for
stakeholders — policy makers and funding agencies in the first place —
to realize that this challenges are challenges we won’t just be able
to solve by hiring standard software engineers. We need more
algorithmics oriented people coding; a need that we share with many
more advanced fields such as AI.</p>

<p><strong>“The broad and multidisciplinary field of quantum software”</strong></p>

<p>I think this is still too restrictive as creating a quantum programming
language or findindg good decoding algorithms for quantum error
correction might not feel covered by this.</p>

<h1 id="comments-on-new-quantum-algorithms">Comments on “New Quantum Algorithms”</h1>

<p><strong>“quantum advantage on small and noisy quantum devices.”</strong></p>

<p>While I of course fully agree with the start of the sentence stating
that the most challenging part is to find algorithms, I’m not sure I
would end it in a way that implicitely conditions the survival of
quantum information processing to our ability to find short term
advantage on small noisy devices.</p>

<p>The reason is that as academics our goal is to find new algorithms and
ways to characterize them. We can of course help the industry in their
quest to applications of quantum technologies in their field. This is
what makes this field so attractive to most of us: being able to do good
science while also creating real-world value. Yet, yielding an advantage
in the sense we usually use it in the field is well outside the realm of
algorithm design, as what will drive the development of quantum
technology is economics, not complexity-, precision-, etc- advantage.
Simply look at GPUs and their thriving development. Do they yield
advantage in terms of complexity? no, but they do yield a economic gain
to their users as they are cheaper to operate per operation than
traditional CPUs.</p>

<p>I would feel more comfortable in saying that throughout our algorithmic
efforts we dedicate to finding new applications both in the long term
and also the short term to quantum machines that will help motivate
further investments in the field yielding better and cheaper harder
hardware.</p>

<h1 id="comments-on-verification-and-testing">Comments on “Verification and Testing”</h1>

<p><strong>“verification and testing protocols will be essential”</strong></p>

<p>This seems to be part of a broader difficulty with programming quantum
computers: the fact that quantum commputers cannot be simulated
efficiently by classical means <em>and</em> that we cannot halt the computation
mid-way to inspect the state of the quantum systems is a double
challenge. This renders our two most powerful classical debugging tools
useless, thereby calling for new methods.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[EQSI has published the Quantum Software Manifesto. It aims to reach out to policy makers and the wider public to emphasize the role of software in the development of quantum information processing capabilities.]]></summary></entry><entry><title type="html">Cryptography in a Quantum World – Paris Rally, 2024 May 23-24</title><link href="http://localhost:4000/2024/05/20/cryptography_in_a_quantum_world.html" rel="alternate" type="text/html" title="Cryptography in a Quantum World – Paris Rally, 2024 May 23-24" /><published>2024-05-20T00:00:00+02:00</published><updated>2024-05-20T00:00:00+02:00</updated><id>http://localhost:4000/2024/05/20/cryptography_in_a_quantum_world</id><content type="html" xml:base="http://localhost:4000/2024/05/20/cryptography_in_a_quantum_world.html"><![CDATA[<p><img src="/assets/CIQWPR2024.jpg" alt="Logo" /></p>

<p>This is an informal rally for the Paris crypto x quantum community to
welcome Claude Crépeau as INRIA’s International Chair Awardee.</p>

<p>The objective of the meeting is to:</p>
<ul>
  <li>get a sense of what everyone is currently doing</li>
  <li>identify and plan possible future collaborations</li>
</ul>

<p>The format purposefully light and relaxed, with 2 to 3 30’ talks per
session and at least 30’ to chat freely after the talks.</p>

<h1 id="location">Location</h1>
<p>The rally will take place in <a href="https://what3words.com/streaks.begun.outdoor">amphi 55B at
Jussieu</a>. For speakers:
It has large blackboards and a video projection system.</p>

<h1 id="food">Food</h1>
<p>There will be coffee breaks outside the amphitheatrer. Lunch is on
your own.</p>

<h1 id="program-final-until-next-revision">Program (final until next revision)</h1>

<table>
  <thead>
    <tr>
      <th>MAY 23</th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10:30</td>
      <td> </td>
      <td>Welcome</td>
    </tr>
    <tr>
      <td>11:00</td>
      <td>Claude Crépeau</td>
      <td>Do unicorns have wings?</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>14:00</td>
      <td>Ivan Supic</td>
      <td>Compiled XOR games</td>
    </tr>
    <tr>
      <td>14:30</td>
      <td>Yoann Piétri &amp; Matteo Schiavon</td>
      <td>Experimental Quantum Cryptography at LIP6</td>
    </tr>
    <tr>
      <td>15:00</td>
      <td>Paul Hilaire</td>
      <td>A spin-optical quantum computing architecture</td>
    </tr>
    <tr>
      <td>15:30</td>
      <td> </td>
      <td>Discussion</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>16:00</td>
      <td>Petros Wallden</td>
      <td>Quantum solutions to the Shortest Vector Problem</td>
    </tr>
    <tr>
      <td>16:30</td>
      <td>Henry Bambury</td>
      <td>Lattice-based cryptanalysis of post-quantum proposals</td>
    </tr>
    <tr>
      <td>17:00</td>
      <td>Leo Colisson</td>
      <td>Round-optimal oblivious transfer without structure</td>
    </tr>
    <tr>
      <td>17:30</td>
      <td> </td>
      <td>Discussion</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>MAY 24</th>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10:00</td>
      <td>Elham Kashefi</td>
      <td>Why Crypto Community should care about Verification of Quantum Computing</td>
    </tr>
    <tr>
      <td>10:30</td>
      <td>Luka Music</td>
      <td>On the necessity to integrate verification early into hardware architectures</td>
    </tr>
    <tr>
      <td>11:00</td>
      <td>Harold Ollivier</td>
      <td>Fault-Tolerant Verification of Quantum Computations</td>
    </tr>
    <tr>
      <td>11:20</td>
      <td>Sami Abdul-Sater</td>
      <td>Circuit model helps MBQC verification, and vice-versa</td>
    </tr>
    <tr>
      <td>12:00</td>
      <td>Maxime Garnier</td>
      <td>Weak Coherent Pulse Secure Delegated Quantum Computation</td>
    </tr>
    <tr>
      <td>12:20</td>
      <td> </td>
      <td>Discussion</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>14:00</td>
      <td>Michael Oliveira</td>
      <td>Heuristic-free Verification-inspired Quantum Benchmarking</td>
    </tr>
    <tr>
      <td>14:30</td>
      <td>Uta Meyer</td>
      <td>Self-Testing Graph States Permitting Bounded Classical Communication</td>
    </tr>
    <tr>
      <td>15:00</td>
      <td>Paul Hermouet</td>
      <td>Unclonable Cryptography in the Plain Model</td>
    </tr>
    <tr>
      <td>15:30</td>
      <td>André Chailloux</td>
      <td>The Quantum Decoding Problem</td>
    </tr>
    <tr>
      <td>16:00</td>
      <td> </td>
      <td>Discussion</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>16:30</td>
      <td>Jessica Bavaresco</td>
      <td>Higher-order operations approach to channel discrimination</td>
    </tr>
    <tr>
      <td>17:00</td>
      <td>Vanessa Brzić &amp; Marco Túlio Quintino</td>
      <td>How to store quantum operations on quantum states</td>
    </tr>
    <tr>
      <td>17:30</td>
      <td>Damian Markham</td>
      <td>Secure networks of sensors</td>
    </tr>
    <tr>
      <td>18:00</td>
      <td> </td>
      <td>Discussion about future collaborations</td>
    </tr>
  </tbody>
</table>

<h1 id="support">Support</h1>
<p>This event is supported by ANR project SecNISQ and</p>

<p><img src="/assets/PCQT-full-txt-bleu.jpg" alt="PCQT" /></p>]]></content><author><name></name></author><category term="Event" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Quantum Programming: Tomography Based Escape Game</title><link href="http://localhost:4000/2023/11/25/escape_tomography.html" rel="alternate" type="text/html" title="Quantum Programming: Tomography Based Escape Game" /><published>2023-11-25T00:00:00+01:00</published><updated>2023-11-25T00:00:00+01:00</updated><id>http://localhost:4000/2023/11/25/escape_tomography</id><content type="html" xml:base="http://localhost:4000/2023/11/25/escape_tomography.html"><![CDATA[<h1 id="context">Context</h1>
<p>A group of scientists (you) is locked in a quantum lab. There is a
machine in one corner of the lab. It seems to be alive as the
following command seems to trigger a reply <code class="language-plaintext highlighter-rouge">curl
"https://quantum-26481f83aff0.herokuapp.com/"</code>.</p>

<p>On one desk there is the manual for the machine. You understand it is
a configurable measurement device that can perform any qubit POVM once
a pure state quantum source is connected to it.</p>

<p>One wall is covered with small boxes labelled by a 16-hex number key
— <code class="language-plaintext highlighter-rouge">xxxxxxxxxxxxxxxx</code> where <code class="language-plaintext highlighter-rouge">x</code>s takes values in
<code class="language-plaintext highlighter-rouge">0,...9,a,b,c,d,e,f</code>. These are the sources than can be connected to
the machine. One has a special label <code class="language-plaintext highlighter-rouge">sandbox</code>.</p>

<p>The machine has 4 modes <code class="language-plaintext highlighter-rouge">p</code>, <code class="language-plaintext highlighter-rouge">m</code>, <code class="language-plaintext highlighter-rouge">n</code>, <code class="language-plaintext highlighter-rouge">u</code> that can be selected. The
manual says:</p>

<p>The <code class="language-plaintext highlighter-rouge">p</code> mode performs a POVM measurement on a given source
<code class="language-plaintext highlighter-rouge">xxxxxxxxxxxxxxxx</code>. The outcome is obtained by sending a <code class="language-plaintext highlighter-rouge">POST</code> request
to <code class="language-plaintext highlighter-rouge">https://quantum-26481f83aff0.herokuapp.com/p/xxxxxxxxxxxxxxxx</code>,
where the payload is a <code class="language-plaintext highlighter-rouge">JSON</code> describing the POVM elements.</p>

<p>The following payload defines a POVM with a single element equal to the identity:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{ "elements" : 
    [
      [[[1,0],[0,0]],[[0,0],[1,0]]]
    ]
}
</code></pre></div></div>

<p>The response is a <code class="language-plaintext highlighter-rouge">JSON</code> looking like:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
'error':   None                 | STRING,
'outcome': SINGLE-ELEMENT ARRAY | None  ,
'trial':   INTEGER              | None  ,
}
</code></pre></div></div>

<p>The number in the outcome corresponds to the index of the corresponding POVM element.</p>

<p>Other modes are accessed in the same way.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">m</code> stands for adding a fixed strength depolarizing noise (i.e. the state is replaced with the maximally mixed state with probability <code class="language-plaintext highlighter-rouge">q</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">n</code> stands for adding classical noise on measurement results (i.e. if there are <code class="language-plaintext highlighter-rouge">n</code> outcomes with probability <code class="language-plaintext highlighter-rouge">1-p</code> it does not change and with probability <code class="language-plaintext highlighter-rouge">p/(n-1)</code> it is being sent to any of the other <code class="language-plaintext highlighter-rouge">n-1</code> outcome.</li>
  <li><code class="language-plaintext highlighter-rouge">u</code> is unfortunately covered under some big coffee stains, but you can still see that:
    <ol>
      <li>it first performs a unitary on the states produced by the source,</li>
      <li>it will add some low strength depolarizing noise</li>
      <li>it adds the same classical noise on the measurement results as when the selector is on <code class="language-plaintext highlighter-rouge">n</code>.</li>
    </ol>
  </li>
</ul>

<h1 id="challenge">Challenge</h1>

<p>Your task is to recover the unitary, the strength of the depolarizing noise and the probability <code class="language-plaintext highlighter-rouge">p</code> for the classical noise… knowing that when you use the <code class="language-plaintext highlighter-rouge">u</code> mode, it will destroy the source after 100 trials, making it useless for any further experiment. The scientists with broken sources will remain forever in “salle R”.</p>

<h1 id="additional-information">Additional information</h1>

<ul>
  <li>You might find a way to cheat. Don’t hesitate to use it.</li>
  <li>You can partner and team up, but choose your firends wisely as they might use your source to make sure you will not escape.</li>
</ul>

<h1 id="sample-code">Sample code</h1>

<p>For accessing the machine with your source:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import json
import requests

key = 'sandbox'
endpoint = 'p' # 'p', 'm', 'n', 'u'
url = 'http://127.0.0.1:8000/' + endpoint +'/' + key

def get_outcome(url):
    response = requests.post(url, json={
        "elements":[
            # element 1: (real, imaginary),
            # element 2: (real, imaginary),
            # ...
            ([[1,0],[0,0]], [[1,-1],[0,0]]),     # This corresponds to [[1+1j, 0-1j],[0+0j, 0+0j]]
            ([[0,0],[0,1]], [[-1,1],[0,0]])      # This corresponds to [[0-1j, 0+1j],[0+0j, 1+0j]]
            ]
        })
    error = response.json()["error"]
    trial = response.json()["trial"]
    outcome = response.json()["outcome"]
    print("Error: ", error)
    print("Trial: ", trial)
    print("Outcome: ", outcome)

get_outcome(url)
</code></pre></div></div>]]></content><author><name></name></author><category term="Lecture" /><summary type="html"><![CDATA[Context A group of scientists (you) is locked in a quantum lab. There is a machine in one corner of the lab. It seems to be alive as the following command seems to trigger a reply curl "https://quantum-26481f83aff0.herokuapp.com/".]]></summary></entry><entry><title type="html">Foundation and tools (of quantum information theory)</title><link href="http://localhost:4000/2023/09/26/foundations_and_tools.html" rel="alternate" type="text/html" title="Foundation and tools (of quantum information theory)" /><published>2023-09-26T00:00:00+02:00</published><updated>2023-09-26T00:00:00+02:00</updated><id>http://localhost:4000/2023/09/26/foundations_and_tools</id><content type="html" xml:base="http://localhost:4000/2023/09/26/foundations_and_tools.html"><![CDATA[<article id="content" class="content">
<header>
<h1 class="title">Foundations and tools</h1>
</header><p>
In this lecture, we introduce the foundational questions of information theory, both in the classical and quantum setting. This serves to emphasize the fact that quantum information theory is really information theory specialized to information carriers that abide by the axioms of quantum mechanics, and as a motivation to develop a set of tools that are useful to tackle more complex tasks.
</p>

<section id="outline-container-org5fd69c8" class="outline-2">
<h2 id="org5fd69c8"><span class="section-number-2">1.</span> State discrimination</h2>
<div class="outline-text-2" id="text-1">
<p>
State discrimination is the task of distinguishing between two known states given a system prepared in one of the two states. More precisely, we consider a setup where there is a sender and a receiver. The sender can prepare one of two states that are also known to the receiver. The receiver gets one such state and decides which of the two states was prepared by the sender.
</p>
</div>
<div id="outline-container-org51428eb" class="outline-3">
<h3 id="org51428eb"><span class="section-number-3">1.1.</span> Classical setting</h3>
<div class="outline-text-3" id="text-1-1">
<p>
In the classical setting, the state can be thought of a one-bit register that is prepared in \(0\) with some probability \(p\) or \(q\) depending whether the sender is preparing one of the two states \(P\) or \(Q\).<label id="fnr.1" for="fnr-in.1.1064233" class="margin-toggle sidenote-number"><sup class="numeral">1</sup></label><input type="checkbox" id="fnr-in.1.1064233" class="margin-toggle" /><span class="sidenote"><sup class="numeral">1</sup>The interest of such task should be clear: the receiver is trying to recover the information about which state was prepared &#x2014; that could be one bit of a message for instance &#x2014; from the signal that he receives.</span> An extreme situation is when \(p = 1\) and \(q = 0\), that is \(P\) is a register with value 0, and \(Q\) is a register with value \(1\). Then it is easy to discriminate between the two upon reading the value of the register itself. Things get a bit more interesting whenever \(p,q \in (0,1)\). What is then the best guess that can be made about the state chosen by the sender?
</p>

<p>
The answer is given by applying Bayes rule after stating our prior on the behavior of the sender. Here, we assume that our initial knowledge about the behavior of the sender is that he can choose to prepare \(P\) or \(Q\) with equal probability, i.e. \(\frac 1 2\). If we denote by \(R\) the random variable that corresponds to the choice of \(P\) or \(Q\), then we have:
</p>
\begin{align}
\Pr(b = 0, R = P) = p/2, &amp; \ \Pr(b = 1, R = P) = (1-p)/2  \\
\Pr(b = 0, R = Q) = q/2, &amp; \ \Pr(b = 1, R = Q) = (1-q)/2.
\end{align}
<p>
This in turn gives<label id="fnr.2" for="fnr-in.2.3788092" class="margin-toggle sidenote-number"><sup class="numeral">2</sup></label><input type="checkbox" id="fnr-in.2.3788092" class="margin-toggle" /><span class="sidenote"><sup class="numeral">2</sup>Remember that \(b \in \{0,1\}\) so that \(\Pr(b|R=P) = (1-b)p + b(1-p)\).</span>:
</p>
\begin{align}
\Pr(R = P | b) &amp; = \frac{\Pr(b, R = P) }{\Pr(b)} = \frac{(1-b)\frac p 2 + b \frac{1-p}{2}}{(1-b)\frac{p+q}{2} + b (1-\frac{p+q}{2})} \\
\Pr(R = Q | b) &amp; = \frac{\Pr(b, R = Q) }{\Pr(b)} = \frac{(1-b)\frac q 2 + b \frac{1-q}{2}}{(1-b)\frac{p+q}{2} + b (1-\frac{p+q}{2})}.
\end{align}
<p>
Given this, upon observing \(b\), the receiver should decide that \(R = P\) whenever \(\Pr(R = P | b) \geq \Pr(R = Q | b)\), i.e. whenever:
</p>
\begin{align}
p-q &amp; \geq 0 \mbox{ for } b = 0\\
 q-p &amp; \geq 0 \mbox{ for } b = 1.
\end{align}
<p>
By denoting \(\hat R\) the random variable that represents the choice of the receiver, we can now assess the success probability \(\Pr(\hat R = R)\). This probability is equal to:
</p>
\begin{align}
Pr(\hat R = R ) &amp; = \Pr(b=0, R = P) \one_{p\geq q} + \Pr(b=1, R = P) \one_{p\leq q} \nonumber \\
&amp; \qquad + \Pr(b=0, R = Q) \one_{q\geq p} + \Pr(b=1, R = Q) \one_{q\leq p} \\
&amp; = \frac{p + 1 - q}{2} \one_{p\geq q} + \frac{1-p + q}{2} \one_{p\leq q}\\
&amp; = \frac 1 2 + \frac 1 2 |p - q|.
\end{align}

<div text="Trace distance for classical probabilities" class="definition" id="org28169bf">
<p>
The trace distance<label id="fnr.3" for="fnr-in.3.9132099" class="margin-toggle sidenote-number"><sup class="numeral">3</sup></label><input type="checkbox" id="fnr-in.3.9132099" class="margin-toggle" /><span class="sidenote"><sup class="numeral">3</sup>The normalization of the trace distance varies depending on authors. It is often normalized using an additional \(\frac{1}{2}\) factor so that the trace distance is between 0 and 1 for probability distributions.</span> between two probability distributions \(P\) and \(Q\) over some event space \(\Gamma\) is:
</p>
\begin{equation}
\| P - Q \|_1 = \sum_{\gamma \in \Gamma} |\Pr(P = \gamma) - \Pr(Q =\gamma)|.
\end{equation}
<p>
For binary variables \(P\) and \(Q\) parametrized by \(p \coloneqq \Pr(P=0)\) and \(q\coloneqq \Pr(Q=0)\) we have \(\|P-Q\|_1 = 2 |p-q|\).
</p>

</div>

<p>
Using this definition, the above success probability for the classical state discrimination is \(\frac{1}{2}(1 + \frac{1}{2}\| P - Q\|_1)\). This means that the trace-distance is an operational measure of similarity between probability distributions. It characterizes how well a receiver can perform the discrimination task presented above, and one recovers the usual intuition that when \(p = 1\) and \(q = 0\), \(\|P - Q \|_1 = 2\) so that the probability of success is 1, while for \(p = q\), both probability distributions are identical so that \(\|P - Q\|_1 = 0\), and the receiver needs to randomly guess the value of \(R\). This intuition can be made more formal by noticing that \(\| \cdot \|_1\) defines a metric over the space of probability distributions:
</p>
<ul class="org-ul">
<li>it is symmetric \(\| P - Q \|_1 = \| Q - P \|_1\)</li>
<li>it is non negative \(\| P-Q \|_1 \geq 0\)</li>
<li>it is definite \(\| P - Q \|_1 = 0 \Rightarrow P = Q\)</li>
<li>and it satisfies the triangle inequality \(\|P - R\|_1 \leq \|P - Q\|_1 + \|Q - R \|_1\).</li>
</ul>
</div>
</div>

<div id="outline-container-org5aac8e0" class="outline-3">
<h3 id="org5aac8e0"><span class="section-number-3">1.2.</span> Quantum setting</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-org89f9259" class="outline-4">
<h4 id="org89f9259"><span class="section-number-4">1.2.1.</span> Trace norm and trace distance for operators</h4>
<div class="outline-text-4" id="text-1-2-1">
<div text="Trace norm for operators" class="definition" id="org5b03d51">
<p>
The <i>trace norm</i> of \(A \in Hom(\mathcal H, \mathcal H')\) is:
</p>
\begin{equation}
\|A\|_1 = \tr(|A|),
\end{equation}
<p>
with \(|A| = \sqrt{A^\dagger A}\).
</p>

</div>

<div class="property" id="org0a7362b">
<p>
The trace norm of \(A\) is the sum of its singular values.
</p>

</div>

<div class="proof" id="org06c8f9a">
<p>
Decompose \(A = U \Delta V\) where \(U\) and \(V\) are unitary matrices and \(\Delta\) is a rectangular diagonal matrix with non-negative (singular) values. Then expanding \(A^\dagger A = V \Delta^\dagger\Delta V\) shows that \(A^\dagger A\) is a Hermitian matrix with singular values equal to the squares of those in \(\Delta\).
</p>

</div>

<p>
It is also easy to show that:
</p>

<div class="property" id="org2d406c2">
<ul class="org-ul">
<li>\(\|A\|_1 \geq 0\) and \(\|A\|_1 = 0 \Leftrightarrow A = 0\);</li>
<li>\(\| \alpha A \|_1 = |\alpha| \| A \|_1\) for \(\alpha \in \mathbb C\);</li>
<li>\(\| U A V^\dagger \|_1 = \| A \|_1\) for \(U\) and \(V\) isometries.</li>
</ul>

</div>

<div class="property" id="orga84f596">
<p>
For \(A \in End(\mathcal H)\),
</p>
\begin{equation} \|A\|_1 = \max_U |\tr(UA)|,\end{equation}
<p>
where \(U\) is a unitary.
</p>

</div>

<div class="proof" id="org24f91be">
<p>
We use the SVD of \(A = V \Delta W\) and write:
</p>
\begin{align}
|\tr(UA) |
&amp; = |\tr(UV\Delta W)| = \tr((\sqrt{\Delta})(\sqrt \Delta W UV)) \\
&amp; \leq \sqrt{\tr(\sqrt \Delta \sqrt \Delta)} \sqrt{\tr ((\sqrt \Delta W UV)^\dagger (\sqrt \Delta W UV))} \\
&amp; = \tr(\Delta) \\
&amp; = \| A \|_1
\end{align}
<p>
where we have used Cauchy-Schwartz inequality<label id="fnr.4" for="fnr-in.4.7315089" class="margin-toggle sidenote-number"><sup class="numeral">4</sup></label><input type="checkbox" id="fnr-in.4.7315089" class="margin-toggle" /><span class="sidenote"><sup class="numeral">4</sup>\(|\langle A,B\rangle| \leq \|A\| \|B\|\) and equality if \(A \propto B\).</span> for the HS inner product. The equality is obtained for \(W U V = \one_{\mathcal H}\).
</p>

</div>

<p>
From there we obtain the triangle inequality:
</p>
<div text="Triangle inequality for the Trace norm" class="property" id="orgcb235c5">
<p>
For \(A,B \in End(\mathcal H)\),
</p>
\begin{equation}
\|A + B \|_1 \leq \|A\|_1 + \|B \|_1.
\end{equation}

</div>
<p>
Note that this holds indeed for \(A,B \in Hom(\mathcal H, \mathcal H')\).
</p>

<div class="proof" id="orga70f8e4">
<p>
We use the variational characterization of the Trace-norm
</p>
\begin{align}
\|A + B\|_1 &amp; = \max_U |\tr(U(A+B))| \\
&amp; = |\tr(\hat U (A+B))| \\
&amp; = |\tr(\hat U A) + \tr(\hat UB)| \\
&amp; \leq |\tr(\hat U A)| + |\tr(\hat UB)| \\
&amp; \leq \max_U |\tr(UA)| + \max_U |\tr(UB)| \\
&amp; = \|A\|_1 + \|B\|_1,
\end{align}
<p>
Where \(\hat U\) is the maximizing \(U\) of the first line.
</p>

</div>

<p>
Using the triangle inequality, we then get the convexity of the trace norm:
</p>
<div class="property" id="org772c2c4">
<p>
For \(A,B \in End(\mathcal H, \mathcal H')\) and \(\lambda \in [0,1]\)
</p>
\begin{equation}
\| \lambda A + (1-\lambda) B \|_1 \leq \lambda \|A\|_1 + (1-\lambda) \|B\|_1.
\end{equation}

</div>

<div text="Trace distance" class="definition" id="orgec9f0c2">
<p>
For \(A,B \in Hom(\mathcal H, \mathcal H')\) the <i>trace-distance</i> between \(A\) and \(B\) is
</p>
\begin{equation}
\|A-B\|_1.
\end{equation}
<p>
For \(\rho\) and \(\sigma\) density matrices we have \(0\leq \|\rho-\sigma\|_1 \leq 2\) &#x2014; which often motivates the use of the <i>normalized</i> trace-distance equal to \(\frac 1 2 \| \rho - \sigma \|_1\).
</p>

</div>
</div>
</div>

<div id="outline-container-org7769943" class="outline-4">
<h4 id="org7769943"><span class="section-number-4">1.2.2.</span> Trace distance as an operational measure of distinguishability</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
The interest of the trace distance for density matrices is its operational interpretation as a measure of the ability to discriminate <i>quantum states</i>.
</p>

<div class="property" id="org991e4e3">
<p>
For \(\rho, \sigma\) density matrices , we have
</p>
\begin{equation}
\frac{1}{2} \|\rho -\sigma\|_1 = \max_{0 \leq K \leq \one} \tr(K(\rho-\sigma))
\end{equation}

</div>

<div class="proof" id="org94b1c41">
<p>
Diagonalize \(\rho-\sigma\) and consider \(\Pi_{&gt;0}\) the projector onto the subspaces with positive eigenvalues and \(\Pi_{&lt;0}\) the projector onto the subspace with negative eigenvalues. Then define \(R\coloneqq \Pi_{&gt;0} (\rho - \sigma) \Pi_{&gt;0}\) and \(S\coloneqq \Pi_{&lt;0} (\sigma - \rho) \Pi_{&lt;0}\) so that \(R,S\) are PSD and \(|\rho-\sigma| = R + S\). Now because the positive and negative eigenvalue subspaces are orthogonal we have
</p>
\begin{equation}
\| \rho - \sigma\|_1 = \tr(|\rho - \sigma|) = \tr(R) + \tr(S).
\end{equation}
<p>
But we also have \(\tr(\rho - \sigma) = 0 = \tr(R) - \tr(S)\) so that \(\| \rho - \sigma\|_1 = 2 \tr(R)\).
</p>

<p>
Now by construction \(\tr(R) = \tr(\Pi_{&gt;0} (\rho - \sigma) \Pi_{&gt;0}) = \tr(\Pi_{&gt;0} (\rho-\sigma))\). Finally, for any \(0 \leq K \leq \one\), we have
</p>
\begin{align}
\tr(K(\rho-\sigma)) &amp; = \tr(K(R-S)) \\
&amp; \leq \tr(K R) \\
&amp; \leq \tr(R) \\
&amp; = \frac{1}{2} \| \rho - \sigma\|_1.
\end{align}
<p>
So that \(R\) is the maximizing \(K\).
</p>

</div>


<p>
The above property states that the trace-distance corresponds to the biggest probability difference for obtaining an outcome in a measurement of \(\rho\) and \(\sigma\). In our state discrimination setting the sender now has the possibility of randomly sending \(\rho\) or \(\sigma\) with equal probability to the receiver. The receiver in turn performs a measurement with POVM elements \(K_0,K_1\) such that if it gets \(K_0\), he declares to have received \(\rho\) and if he gets \(K_1\) he declares \(\sigma\). In such case, the success probability is:
</p>
\begin{equation}
\frac 1 2 \tr(K_0 \rho) + \frac 1 2 \tr(K_1 \sigma).
\end{equation}
<p>
Using \(K_0 + K_1 = \one\), we obtain that the success probability is
</p>
\begin{equation}
\frac 1 2 (1 + \tr (K_0 (\rho-\sigma)))
\end{equation}
<p>
Optimizing over \(K_0\) can be done using the above property, and the maximizing \(K_0\) is the projection onto the positive eigenvalue subspace of \(\rho - \sigma\). In such case, we then obtain a success probability of
</p>
\begin{equation}
\frac 1 2 \left( 1 + \frac 1 2 \| \rho - \sigma \|_1\right),
\end{equation}
<p>
which corresponds to the generalization of the previous result for classical probability distributions.
</p>
</div>
</div>
</div>
</section>

<section id="outline-container-orgee4d209" class="outline-2">
<h2 id="orgee4d209"><span class="section-number-2">2.</span> Channel discrimination</h2>
<div class="outline-text-2" id="text-2">
<p>
Channel discrimination is another foundational information theoretic task that enlights some of the differences between quantum and classical information. There, we again have two parties, but in a slightly different setting: the first one, say Alice, is preparing a quantum system in state \(\rho\), the second one, say Bob, applies to it a CPTP map at random from a set \(\{\mathcal E, \mathcal F\}\) and gives back the quantum system to Alice, which then measures it. The task of Alice is to discriminate between \(\mathcal E\) and \(\mathcal F\).
</p>
</div>

<div id="outline-container-orgf38c759" class="outline-3">
<h3 id="orgf38c759"><span class="section-number-3">2.1.</span> Naive approach</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Channel discrimination can be cast into state discrimination. After all, Alice could consider that she receives at random either \(\mathcal E(\rho)\) or \(\mathcal F(\rho)\) which she can discriminate using the optimal strategy seen above. The maximum success probability in this case is:
</p>
\begin{equation}
\frac 1 2 \left ( 1 + \frac 1 2 \| \mathcal E(\rho) - \mathcal F(\rho) \|_1 \right).
\end{equation}
<p>
Because Alice can optimize over \(\rho\), we would naturally be interested in
</p>
\begin{equation}
\max_{\rho} \frac 1 2 \left ( 1 + \frac 1 2 \| \mathcal E(\rho) - \mathcal F(\rho) \|_1 \right),
\end{equation}
<p>
which points at \(\max_\rho \| \mathcal E(\rho) - \mathcal F(\rho) \|_1\) as a measure of distinguishability between \(\mathcal E\) and \(\mathcal F\).
</p>
</div>
</div>

<div id="outline-container-org8bf8b55" class="outline-3">
<h3 id="org8bf8b55"><span class="section-number-3">2.2.</span> Super-dense coding</h3>
<div class="outline-text-3" id="text-2-2">
<p>
In fact, there is more that can be done in order to discriminate channels.
</p>

<p>
To get an intuition we will consider the following channels: \(\mathcal E(\cdot) \coloneqq I(\cdot)I\) and \(\mathcal F(\cdot) \coloneqq X(\cdot) X\), One can perform the optimization above very easily and obtain that the optimal state to construct is \(\ket 0\) and the measurement is \(\{\ketbra 0, \ketbra 1\}\). The success probability is 1 as the resulting state is \(\ket 0\) for \(\mathcal E\) and \(ket 1\) for \(\mathcal F\), which are perfectly distinguishabel.
</p>

<p>
In a similar fashion, if \(\mathcal F(\cdot) \coloneqq Z(\cdot)Z\), then the optimal state is \(\ket +\) and the measurement is \(\{\ketbra +, \ketbra - \}\). The second case can actually be seen as the first one rotated by a Hadamard gate<label id="fnr.5" for="fnr-in.5.3271361" class="margin-toggle sidenote-number"><sup class="numeral">5</sup></label><input type="checkbox" id="fnr-in.5.3271361" class="margin-toggle" /><span class="sidenote"><sup class="numeral">5</sup>\(H\ket 0 = \ket +\) and \(H\ket 1 = \ket -\).</span>, and from there it is clear that using the naïve approach there is no single initial state and optimal measurement that can distinguish the 3 channels \(I, X, Z\) &#x2014; and a fortiori \(I, X, Y, Z\) &#x2014; with probability of success equal to 1.
</p>

<p>
Yet, there is a situation where discriminating between \(I,X,Y,Z\) is possible with success probability 1, which can be derived from the existence of the super dense coding protocol.
</p>

<div text="Super dense coding" class="definition" id="orgd4ece8e">
<p>
Super dense coding is a protocol allowing a sender (Alice) to send a 2-bit message \(m \in \{0,1,2,3\}\) to a receiver (Bob) by sending half a pre-established EPR-pair through a perfect quantum channel.
</p>

<ol class="org-ol">
<li>Alice and Bob share an EPR pair \(\frac 1 {\sqrt 2} \left(\ket{00} + \ket{11}\right)\);</li>
<li>Alice chooses \(m\) and depending on its value she applies \(I,X,Y,Z\) to her half of the EPR-pair, and sends it to Bob</li>
<li>Bob performs a Bell State Measurement on the two qubits (the received half and his own half)</li>
<li>He sets the received message \(\hat m\) depending on the observed Bell state:</li>
</ol>
\begin{align}
\frac{1}{\sqrt 2}\left(\ket{00} + \ket{11}\right) &amp; \rightarrow \hat m = 0 \\
\frac{1}{\sqrt 2}\left(\ket{00} - \ket{11}\right) &amp; \rightarrow \hat m = 3 \\
\frac{1}{\sqrt 2}\left(\ket{10} + \ket{01}\right) &amp; \rightarrow \hat m = 1 \\
\frac{1}{\sqrt 2}\left(\ket{10} - \ket{10}\right) &amp; \rightarrow \hat m = 2
\end{align}

</div>

<p>
The interest of this protocol for channel discrimination comes from the fact that we can imagine that Alice chooses one of the \(I,X,Y,Z\) channel to apply to half an EPR-pair prepared by Bob, who then performs a measurement that perfectly distinguishes the 4 possibilities. This obviously violates the conclusion we drew from the naïve approach. What is the special ingredient that allows this? The use of EPR-pair, i.e. a maximally entangled state, for which one half acts as a reference. The naïve approach didn't allow this as the channel was implicitely applied to the whole state, while here it is only applied partially. In fact, this shouldn't come too much as a surprise as we have seen that a channel is characterized by its Choi state, obtained by applying the channel onto half a maximally entangled state.
</p>
</div>
</div>

<div id="outline-container-org33f350b" class="outline-3">
<h3 id="org33f350b"><span class="section-number-3">2.3.</span> Diamond norm</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Given the intuition built through the previous example, we realize that the optimization over \(\rho\) is not enough. It should be supplemented by an optimization over the size of the Hilbert-space so that the success probability can be written:
</p>
\begin{equation}
\frac 1 2 \left ( 1 + \frac 1 2 \sup_{\mathcal H_R} \max_{\rho} \| (\mathcal E \otimes \one_{\mathcal H_R})(\rho) - (\mathcal F \otimes \one_{\mathcal H_R})(\rho) \|_1\right ),
\end{equation}
<p>
where \(\mathcal E, \mathcal F\) are CPTP maps from \(\mathcal H\) to \(\mathcal H'\) and \(\rho\) is a density matrix over \(\mathcal H \otimes \mathcal H_R\).
</p>

<p>
This defines an induced<label id="fnr.6" for="fnr-in.6.8629263" class="margin-toggle sidenote-number"><sup class="numeral">6</sup></label><input type="checkbox" id="fnr-in.6.8629263" class="margin-toggle" /><span class="sidenote"><sup class="numeral">6</sup>This means a norm over states is used to define a norm over operators using some optimization procedure</span> norm over channels:
</p>
<div text="Diamond norm" class="definition" id="orged06e9f">
<p>
For \(\mathcal E\) a CPTP map from \(\mathcal H\),
</p>
\begin{equation}
\| \mathcal E \|_{\diamond} \coloneqq \max_{\ket \psi \in \mathcal H \otimes \mathcal H} \| (\mathcal E \otimes \one_{\mathcal H})(\ketbra \psi) \|_1
\end{equation}

</div>

<div class="remark" id="org98a0196">
<p>
Note that above it is enough to optimize over pure states &#x2014; using the convexity of the trace distance. Additionally, the optimization over \(\mathcal H_R\) that was expected is omitted in the above definition as it is indeed sufficient to consider \(\mathcal H_R= \mathcal H\) using the fact that the Schmidt rank of a state over \(\mathcal H \otimes \mathcal H_R\) is bounded by \(\dim \mathcal H\).
</p>

</div>

<p>
Finally, and similarly to the trace-distance, one can define the <i>diamond distance</i> between CPTP maps:
</p>
<div text="Diamond distance" class="definition" id="orgb0d0a2b">
<p>
For \(\mathcal E, \mathcal F\) two CPTP maps from \(\mathcal H\) to \(\mathcal H'\), the <i>diamond distance</i> between them is
</p>
\begin{equation}
\| \mathcal E - \mathcal F \|_\diamond
\end{equation}

</div>
</div>
</div>
</section>

<section id="outline-container-org6dd3e6e" class="outline-2">
<h2 id="org6dd3e6e"><span class="section-number-2">3.</span> Other measures of distance</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org1b2788f" class="outline-3">
<h3 id="org1b2788f"><span class="section-number-3">3.1.</span> Fidelity</h3>
<div class="outline-text-3" id="text-3-1">
<p>
We have seen that the trace-distance arises quite naturally from the state discrimination task. One might also argue that another well motivated notion of distance between two pure quantum states \(\ket \psi, \ket \varphi\) is the squared overlap between the states: \(|\braket{\psi}{\varphi}|^2\). It arises from the following protocol. Again a sender prepares either \(\ket \psi\) or \(\ket \varphi\), and the receiver tries to distinguish one from the other. To do that, instead of using the optimal measurement seen earlier, he could decide to perform the measurement \(\{\ketbra \psi, \one - \ketbra \psi\}\). If the first outcome is obtained, it will decide that \(\ket \psi\) was send, and \(\ket \varphi\) otherwise. The probability of error given \(\ket \varphi\) is sent is therefore \(\tr(\ketbra \psi \ketbra \varphi) = |\braket{\psi}{\varphi}|^2\). Hence, the higher the fidelity, the harder to discriminate \(\ket \psi\) from \(\ket \varphi\) in this test.
</p>

<div text="Pure state fidelity" class="definition" id="orgf11cecf">
<p>
The <i>pure state fidelity</i> for \(\ket \psi, \ket \varphi \in \mathcal H\) is defined as:
</p>
\begin{equation}
F(\ket \psi, \ket \varphi) \coloneqq |\braket{\psi}{\varphi}|^2
\end{equation}

</div>

<div class="property" id="org0309529">
\begin{align}
&amp; F(\ket\psi, \ket \varphi) = F(\ket \varphi, \ket\psi) \\
&amp; 0\leq F(\ket\psi,\ket\varphi) \leq 1
\end{align}

</div>

<div class="remark" id="orgd963aaf">
<p>
There is a notion of classical fidelity for probability distributions \(p(x), q(x)\) called <i>Bhattacharyya overlap</i><label id="fnr.7" for="fnr-in.7.6606673" class="margin-toggle sidenote-number"><sup class="numeral">7</sup></label><input type="checkbox" id="fnr-in.7.6606673" class="margin-toggle" /><span class="sidenote"><sup class="numeral">7</sup>Show that for classical states expressed using quantum systems, the two correspond</span>:
</p>
\begin{equation}
F(p,q) = \left(\sum_x \sqrt{p(x)q(x)}\right)^2.
\end{equation}

</div>

<p>
Using this definition for pure states, it would be useful to extend it to mixed states. As we will see this is not so easy in the general case, so we can build some intuition first. So we start for the pure-mixed case, i.e. the fidelity \(F(\ket\psi,\rho)\). In such case, one way to recover the pure-state fidelity is to consider \(\rho\) as a statistical mixture of the states appearing in its spectral decomposition. More precisely, we define the <i>expected fidelity</i> of \(\rho\) against \(\ket\psi\) as the expectation value of the pure state fidelity considering that \(\rho\) is prepared from its spectral decomposition:
</p>
\begin{equation}
F(\ket\psi,\rho) \coloneqq \sum_\lambda p_\lambda |\braket{\psi}{\varphi_\lambda}|^2 = \bra \psi \rho \ket \psi,
\end{equation}
<p>
where \(\rho = \sum_\lambda p_\lambda \ketbra{\varphi_\lambda}\) with \(\{\ket{\varphi_\lambda}\}_\lambda\) an orthonormal set.
</p>

<p>
The crucial ingredient in the above generalization is that the spectral decomposition offers a natural way to recover pure states fidelity only, and a way to recombine each of the individual fidelities. With this in mind, a possibility &#x2014; that is actually correct &#x2014; would be to rely on the purification of mixed states to recover the pure state situation. This defined the (Uhlmann) fidelity, tht works for mixed states and coincide with the pure state and expected fidelity whenever we are in a pure-pure or pure-mixed setting:
</p>
<div text="Uhlmann Fidelity" class="definition" id="org237f3bb">
<p>
For \(\rho, \sigma\) density matrices over \(\mathcal H_A\) and \(\ket \psi, \ket \varphi \in \mathcal H_A \otimes \mathcal H_R\) such that \(\tr_R (\ketbra \psi) = \rho\) and \(\tr_R( \ketbra \varphi) = \sigma\), the Uhlmann fidelity is<label id="fnr.8" for="fnr-in.8.7668349" class="margin-toggle sidenote-number"><sup class="numeral">8</sup></label><input type="checkbox" id="fnr-in.8.7668349" class="margin-toggle" /><span class="sidenote"><sup class="numeral">8</sup>Remember that (i) purifications are equivalent up to a unitary on the reference system part &#x2014; hence starting from \(\ket \psi\) and \(\ket \varphi\) one can obtain all of them by applying \(U\) for \(\ket \psi\) and \(V\) for \(\ket \varphi\) &#x2014; and (ii) that maximizing over \(U\) and \(V\) is overkill as they impact the value to optimize through  \(U^\dagger V\) which is a unitary itself.</span>
</p>
\begin{equation}
F(\rho,\sigma) = \max_{U}|\bra\psi (\one_A \otimes U) \ket \varphi|^2.
\end{equation}

</div>

<p>
Fortunately, Uhlmann's theorem allows for a closed-form expression of the fidelity:
</p>
<div text="Uhlmann's theorem" class="theorem" id="org18db6a3">
<p>
For \(\rho,\sigma\) density matrices over \(\mathcal H_A\),
</p>
\begin{equation}
F(\rho,\sigma) = \| \sqrt \rho \sqrt \sigma \|_1^2 = \left (\tr(\sqrt{\sqrt\sigma \rho \sqrt \sigma})\right)^2
\end{equation}

</div>

<div class="proof" id="org329b3d6">
<p>
Given \(\rho\) we can take the purification \(\ket \psi = (\sqrt{\rho} \otimes \one_R) \ket\Gamma\) where \(\ket\Gamma = \sum_i \ket{i}_A\otimes \ket{i}_R\) is a unnormalized maximally entangled state over \(\mathcal H_A \otimes \mathcal H_R\), and similarly for \(\sigma\). Then<label id="fnr.9" for="fnr-in.9.8429835" class="margin-toggle sidenote-number"><sup class="numeral">9</sup></label><input type="checkbox" id="fnr-in.9.8429835" class="margin-toggle" /><span class="sidenote"><sup class="numeral">9</sup>It is easy to show that for any \(A\in End(\mathcal H)\), \((A\otimes\one) \ket\Gamma = (\one\otimes A^T)\ket\Gamma\) with \(\ket\Gamma\) the maximally entangled state on \(\mathcal H \otimes \mathcal H\) by directly expressing the matrix elements \(\ketbra{i}{j}\). It is also straightforward using the same appraoch to show that \(\tr(A) = \bra\Gamma (A\otimes \one) \ket \Gamma\).</span> we have for any \(U\):
</p>
\begin{align}
|\bra \psi (\one_A \otimes U) \ket \varphi |^2 &amp; = |\bra{\Gamma} (\sqrt{\rho}\sqrt{\sigma} \otimes U) \ket \Gamma|^2 \\
&amp; |\bra \Gamma (\sqrt{\rho}\sqrt{\sigma}U^T \otimes \one_R \ket \Gamma\|^2 \\
&amp; = |\tr(\sqrt\rho \sqrt\sigma U^T)|^2.
\end{align}
<p>
Now maximizing \(U\) and using the variational characterization of the trace-distance we get:
</p>
\begin{equation}
\max_U |\tr(\sqrt\rho \sqrt\sigma U^T)|^2 = \|\sqrt\rho\sqrt\sigma\|_1^2.
\end{equation}

</div>

<div class="property" id="org2d17c75">
<ul class="org-ul">
<li>\(F\) is symmetric</li>
<li>\(F\) is multiplicative with respect to tensor products: \(F(\rho_A\otimes \rho_B, \sigma_A\otimes \sigma_B) = F(\rho_A, \sigma_A)F(\rho_B, \sigma_B)\)</li>
<li>\(F\) is invariant under isometries</li>
<li>\(F\) is non-decreasing over partial trace<label id="fnr.10" for="fnr-in.10.9070986" class="margin-toggle sidenote-number"><sup class="numeral">10</sup></label><input type="checkbox" id="fnr-in.10.9070986" class="margin-toggle" /><span class="sidenote"><sup class="numeral">10</sup>The idea is to use the variational characterization of \(F\) and realize that the lhs optimizes over unitaries on the reference system, while the rhs optimizes over unitaries on the reference system and \(B\). This means that forgetting about one part of the system makes it easier to take one state for the other. This is generalized in the next property. There, if you see \(\mathcal E\) as a possibly noisy evolution, it will improve the fidelity between the states. This kind of inequality is also referred to as <i>Data processing inequality</i>. Such inequality apply to various kind of quantities and are very useful in characterizing the evolution of soem quantities under noisy processes.</span>: \(F(\rho_{AB}, \sigma_{AB}) \leq F(\rho_A, \sigma_A)\)</li>
<li>\(F\) is monotone under CPTP maps: \(F(\rho,\sigma) \leq F(\mathcal E (\rho), \mathcal E(\sigma))\)</li>
<li>\(\sqrt F\) is jointly concave with respect to its arguments: \(\sqrt F(\lambda \rho + (1-\lambda) \rho', \lambda \sigma + (1-\lambda) \sigma') \geq \lambda\sqrt F(\rho,\sigma) + (1-\lambda) \sqrt F(\rho',\sigma')\)</li>
<li>\(F\) is concave wrt one of its arguments: \(F(\lambda \rho + (1-\lambda)\rho',\sigma) \geq \lambda F(\rho, \
  \sigma) + (1-\lambda) F(\rho', \sigma)\)</li>
</ul>

</div>
</div>
</div>
</section>
<section id="outline-container-org50cab28" class="outline-2">
<h2 id="org50cab28"><span class="section-number-2">4.</span> Classical entropies and information</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org8f592e8" class="outline-3">
<h3 id="org8f592e8"><span class="section-number-3">4.1.</span> Overview of the operational definition of information and entropy</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Information is a concept that was quantified by Shannon in 1948. As for distance measures, it stems out of a simple foundational task involving sending messages from Alice to Bob. This task is called <i>source coding</i> and corresponds to Alice sending messages seen as random bit strings picked at random from a given distribution &#x2014; it could be for instance random words from the dictionary, random sentences from a book, or random paragraphs from a corpus of texts <label id="fnr.11" for="fnr-in.11.6529356" class="margin-toggle sidenote-number"><sup class="numeral">11</sup></label><input type="checkbox" id="fnr-in.11.6529356" class="margin-toggle" /><span class="sidenote"><sup class="numeral">11</sup>Think about chatGPT like situations where the entire web was scraped to gain knowledge about correlations between sentences.</span>. The objective of this task is to encode the message in such a way so that Alice sends the minimum number of bits to Bob, while still allowing Bob to decode the message perfectly. That is we want to find the best possible compression for the source emitting the messages.
</p>

<p>
Shannon's source coding theorem states the following.  Let a source emit words out of an alphabet \(\Sigma\) &#x2014; say \(\{A,B,C,\ldots\}\). Consider \(f\) an encoding of these words into words over an alphabet \(\Pi\) &#x2014; say \(\{0, 1\}\) &#x2014; that is uniquely decodable<label id="fnr.12" for="fnr-in.12.9014515" class="margin-toggle sidenote-number"><sup class="numeral">12</sup></label><input type="checkbox" id="fnr-in.12.9014515" class="margin-toggle" /><span class="sidenote"><sup class="numeral">12</sup>This means that any input word is mapped to a single output codeword</span>. Then we can consider an optimal \(f\) that is such that when \(X\) is drawn at random from the words on \(\Sigma\) according to a known probability distribution, the expected length of the encoding \(|f(X)|\) is minimal &#x2014; i.e. we consider \(f\) such that \(\mathbb E(|f(X)|)\) is miminum. Then
</p>
\begin{equation}
\frac{H(X)}{\log_2(|\Pi|)} \leq \mathbb{E}[|f(X)|]  &lt; \frac{H(X)}{\log_2(|\Pi|)} + 1,
\end{equation}
<p>
where \(H(X) \coloneqq - \sum_x \Pr(X=x)\log_2(\Pr(X=x))\) is the entropy of \(X\). In other words, this theorem states that the best encoding \(f\) will produce codewords whose expected length is given by \(H(X)/\log_2(|\Pi|)\). If \(\Pi\) is taken to be bits, then \(H(X)\) directly measures the expected length of the codewords. This means that the average information content of \(X\) is \(H(X)\) bits as it is not possible to reduce the length of the codewords any further<label id="fnr.13" for="fnr-in.13.9959021" class="margin-toggle sidenote-number"><sup class="numeral">13</sup></label><input type="checkbox" id="fnr-in.13.9959021" class="margin-toggle" /><span class="sidenote"><sup class="numeral">13</sup>Lets beat that bound! Consider meassages made out of the 26 letters and 10 numerals, chosen uniformly at random. Consider the <a href="https://en.wikipedia.org/wiki/Morse_code#/media/File:International_Morse_Code.svg">Morse encoding</a>. We have \(H(X) = - \log_2(1/36) = 5.16\), while we see that all letters are encoded using at most 4 symbols and numerals exactly 5. Hence, we have \(\mathbb{E}(|f(X)|) \leq 4 \times \frac{26}{36} + 5 \times \frac{10}{36} = 4.22\). What's wrong?</span>
</p>

<p>
A second important task put forth by Shannon is called <i>channel coding</i>. It corresponds to Alice sending messages to Bob, but this time through a noisy channel. The goal is then to find out how to encode the message, possibly adding some redundancy, so that Bob can perfectly retreive the message of Alice with very high probability.
The important quantity that is highlighted by this task is the mutual information, which is derived from entropy (see below).
</p>

<p>
Here, instead of going through the proofs of Shannon's theorems &#x2014; i.e. source and channel coding &#x2014; we will take an axiomatic approach to defining entropy, yet I encourage you to read Shannon's paper and see the concept of entropy and mutual information emerge from these operational tasks.
</p>
</div>
</div>

<div id="outline-container-orgf647f8d" class="outline-3">
<h3 id="orgf647f8d"><span class="section-number-3">4.2.</span> Axiomatic definition</h3>
<div class="outline-text-3" id="text-4-2">
</div>
<div id="outline-container-org5ab3bfe" class="outline-4">
<h4 id="org5ab3bfe"><span class="section-number-4">4.2.1.</span> Entropy of events</h4>
<div class="outline-text-4" id="text-4-2-1">
<p>
We consider  \((\Omega, \mathcal E, P)\) a probability space<label id="fnr.14" for="fnr-in.14.1111253" class="margin-toggle sidenote-number"><sup class="numeral">14</sup></label><input type="checkbox" id="fnr-in.14.1111253" class="margin-toggle" /><span class="sidenote"><sup class="numeral">14</sup>\(\Omega\) is the sample space &#x2014; the set of possible outcomes &#x2014; \(\mathcal E\) is the event space &#x2014; each event is a set of outcomes &#x2014; and \(P\) a probability function which assigns a probability to each element in \(\mathcal E\)</span>. The entropy of an event \(E\) is a function over \(\mathcal E\):
</p>
\begin{align}
H:\
&amp;  \mathcal E \rightarrow \mathbb R \cup \{\infty\} \nonumber \\
&amp; E \mapsto H(E) \nonumber
\end{align}
<p>
that satisfies:
</p>
<ol class="org-ol">
<li><i>independence</i> of representation: \(H(E)\) only depends on \(P(E)\)</li>
<li><i>continuity</i> (relative to the topology induced by the trace distance)</li>
<li><i>additivity</i> for two independent events \(E\) and \(E'\): \(H(E \cap E') = H(E) + H(E')\)</li>
<li><i>normalization</i> such that \(H(E) = 1\) for \(P(E) = \frac 1 2\)</li>
</ol>

<p>
With these constraints, the entropy of \(E\) is unique:
</p>
<div class="theorem" id="orgf40f74a">
<p>
Any fonction \(H\) satisfying 1-4 above has the form \(H(E) = -\log_2(P(E))\)
</p>

</div>

<div class="proof" id="org72a8faa">
<p>
Clearly \(-\log_2(P(E))\) satisfies 1-4. Because \(-\log_2\) is bijective from \([0,1]\) to \(\mathbb R^+ \cup \{+\infty\}\), the condition 1 allows to write \(H(E) = f(-\log_2(P(E)))\), where \(f\) is from \(\mathbb R^+ \cup \{+\infty\}\) to \(\mathbb R \cup \{+\infty\}\). 2 implies that \(f\) is continuous and additivity implies that \(f(x) + f(y) = f(x + y)\) by considering two events \(E\) and \(E'\) such that \(-\log_2(P(E)) = x\) and \(-\log_2(P(E')) = y\). This in turn implies that \(f\) is linear, and the normalization implies the claimed result.
</p>

</div>
</div>
</div>

<div id="outline-container-org99e6c41" class="outline-4">
<h4 id="org99e6c41"><span class="section-number-4">4.2.2.</span> Entropies for random variables</h4>
<div class="outline-text-4" id="text-4-2-2">
<p>
The most obvious way to go from events to random variables is to consider the expected entropy for each event defining the random variable, i.e.
</p>
<div text="Shannon Entropy" class="definition" id="org794f9c9">
<p>
Shannon entropy of a random variable \(X\) with alphabet \(\Sigma\) is the expected entropy of the events \(\{X = \sigma\}\) for \(\sigma \in \Sigma\):
</p>
\begin{equation}
H(X) = - \sum_{\sigma\in \Sigma} \Pr(X=\sigma) \log_2(\Pr(X=\sigma)).
\end{equation}

</div>

<p>
But this is not the only possible way of combining the various probabilities of the events \(\{X=\sigma\}\):
</p>
<div text="Min-Entropy" class="definition" id="orgb116a98">
<p>
The Min-entropy of a random variable \(X\) with alphabet \(\Sigma\) is
</p>
\begin{equation}
H_{\min}(X) = \min_\sigma - \log_2(\Pr(X=\sigma)).
\end{equation}

</div>

<p>
It is also possible to define the Max-entropy, that is related to the number of events with non-zero probability<label id="fnr.15" for="fnr-in.15.2045346" class="margin-toggle sidenote-number"><sup class="numeral">15</sup></label><input type="checkbox" id="fnr-in.15.2045346" class="margin-toggle" /><span class="sidenote"><sup class="numeral">15</sup>This measures the maximum entropy a random variable with the same support as \(X\) could have.</span>:
</p>

<div text="Max-Entropy" class="definition" id="org2792c62">
<p>
The Max-entropy of a random variable \(X\) with alphabet \(\Sigma\) is
</p>
\begin{equation}
H_{\max}(X) = \log_2(|\{X=\sigma \ : \ \Pr(X=\sigma) &gt; 0\}|).
\end{equation}

</div>

<div class="property" id="org719db20">
\begin{equation}
H_{\min}(X) \leq H(X) \leq H_{\max}(X).
\end{equation}

</div>

<div class="property" id="orgb19e067">
<p>
For all entropies (min, max, Shannon), we have
</p>
<ul class="org-ul">
<li>Permutation invariance: \(H(X) = H(\pi(X))\) where \(\pi\) permutes the elements of \(X\);</li>
<li>Non negativity;</li>
<li>Equality to \(0\) is equivalent to having exactly one element with probability 1;</li>
<li>Upper bound equal to \(\log_2|\Sigma|\) where \(\Sigma\) is the alphabet.</li>
</ul>

</div>
</div>
</div>

<div id="outline-container-orgf582d6f" class="outline-4">
<h4 id="orgf582d6f"><span class="section-number-4">4.2.3.</span> More entropies</h4>
<div class="outline-text-4" id="text-4-2-3">
<p>
Whenever there are correlated random variables \(X\) and \(Y\), one can define a conditional random variables when a say specific value of \(Y=y\) is known. We write this random variable as \(X|Y=y\) and its probability distribution is \(\Pr(X|Y=y) = \frac{\Pr(X,Y=y)}{\Pr(Y=y)}\). The associated Shannon entropy is \(H(X|Y=y) = -\sum_{x} \Pr(X=x|Y=y) \log_2(\Pr(X=x|Y=y)\). This represents the entropy of \(X\) when \(Y=y\). The conditional Shannon entropy of \(X\) given \(Y\) is defined as this quantity averaged over the possible values of \(y\) and thus represents the expected remaining uncertainty about \(X\) when \(Y\) is known:
</p>
<div text="Conditional Shannon Entropy" class="definition" id="org6c02028">
<p>
The <i>conditional Shannon entropy</i> of \(X\) given \(Y\) is
</p>
\begin{equation}
H(X|Y) = \sum_y \Pr(Y=y) H(X|Y=y).
\end{equation}

</div>

<div text="Chain Rule" class="property" id="orgae591c0">
<p>
The Shannon entropy satisfies the <i>chain rule</i><label id="fnr.16" for="fnr-in.16.5359844" class="margin-toggle sidenote-number"><sup class="numeral">16</sup></label><input type="checkbox" id="fnr-in.16.5359844" class="margin-toggle" /><span class="sidenote"><sup class="numeral">16</sup>This allows to draw Venn diagrams for entropies.</span>:
</p>
\begin{equation}
H(X|YZ) = H(XY|Z) - H(Y|Z)
\end{equation}

</div>

<p>
Conditional entropies can be defined for the min and max entropy as well. For the min entropy we simply minimize over all possible realizations of \(Y\), while for the max entropy, we maximize over the possible values of \(Y\):
</p>
<div text="Conditional Min Entropy" class="definition" id="org63bae89">
\begin{equation}
H_{\min}(X|Y) = \min_y H_{\min}(X|Y=y).
\end{equation}

</div>

<div text="Conditional Max Entropy" class="definition" id="orge1ed7ab">
\begin{equation}
H_{\max}(X|Y) = \max_y \log_2 |\{x: \Pr(X=x|Y=y) \neq 0\}|.
\end{equation}

</div>

<p>
All three entropies satisfy an important and powerful inequality stating that conditional entropies can only decrease when they are conditioned on an additional random variable:
</p>
<div text="Strong Subadditivity" class="property" id="org2802d63">
<p>
H(X|Y) &ge; H(X|YZ).
</p>

</div>
<p>
While such property is expected &#x2014; the entropy of \(X\) decreases when we add more conditioning &#x2014; it is important to mention it here, as the quantum analogue to it  is non-trivial.
</p>
</div>
</div>

<div id="outline-container-orgc49e945" class="outline-4">
<h4 id="orgc49e945"><span class="section-number-4">4.2.4.</span> Mutual information</h4>
<div class="outline-text-4" id="text-4-2-4">
<p>
Mutual information is a concept that closely follows our everyday experience. When we get information, we get it through something about something else: We receive a message telling us that some event happened. It is not directly the event itself, but it is correlated to it in the sense that if the event would have been different so would have been the message.
</p>

<p>
Formally, the Shannon mutual information between \(X\) and \(Y\) is the average amount of uncertainty about \(X\) that \(Y\) removes when it is observed:
</p>
<div text="Mutual Information" class="definition" id="orga8fb369">
\begin{equation}
I(X:Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(XY).
\end{equation}

</div>

<div class="property" id="orga8f7bb6">
<p>
The mutual information is non-negative.<label id="fnr.17" for="fnr-in.17.2863221" class="margin-toggle sidenote-number"><sup class="numeral">17</sup></label><input type="checkbox" id="fnr-in.17.2863221" class="margin-toggle" /><span class="sidenote"><sup class="numeral">17</sup>Use the strong subadditivity.</span>
</p>

</div>
</div>
</div>
</div>
</section>

<section id="outline-container-org7ad72e6" class="outline-2">
<h2 id="org7ad72e6"><span class="section-number-2">5.</span> Quantum entropies</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-orgdf59019" class="outline-3">
<h3 id="orgdf59019"><span class="section-number-3">5.1.</span> Generalizing classical entropies</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Surely, it is possible to encode classical information &#x2014; i.e. classical probability distributions &#x2014; inside quantum systems. This holds for single systems as well as for correlated ones. Because we could also perform all classical processing within quantum systens, it is ultimately desirable that the definition of quantum entropy as well as conditional entropy, mutual information and relative entropy match those of random variables whenever the quantum states correspond to random variables.
</p>

<p>
This leads to the following definitions:
</p>
<div text="Von Neuman Entropy" class="definition" id="org0fc33df">
<p>
The von Neuman entropy for a system \(A\) in state \(\rho\) is<label id="fnr.18" for="fnr-in.18.1012379" class="margin-toggle sidenote-number"><sup class="numeral">18</sup></label><input type="checkbox" id="fnr-in.18.1012379" class="margin-toggle" /><span class="sidenote"><sup class="numeral">18</sup>Show that is matches the classical definition for classical states.</span>:
</p>
\begin{equation}
H(A)_\rho = - \tr(\rho \log_2 \rho).
\end{equation}

</div>

<div text="quantum min Entropy" class="definition" id="org6810240">
<p>
The min entropy is defined using the $&infin;$-norm &#x2013; equating the largest eigenvalue of \(\rho\), by:
</p>
\begin{equation}
H_{\min}(A)_\rho = -\log_2 \|\rho\|_{\infty}
\end{equation}

</div>

<div text="quantum max Entropy" class="definition" id="org02002f7">
<p>
The max entropy is defined by
</p>
\begin{equation}
H_{\max}(A)_\rho = -\log_2 |\mathrm{supp}(\rho)|,
\end{equation}
<p>
where \(\mathrm{supp}(\rho)\) is the orthogonal complement to the \(0\) eigenspace value of \(\rho\).
</p>

</div>

<p>
Entropies satisfy useful properties:
</p>
<div class="property" id="org83152f7">
<ul class="org-ul">
<li>Unitary invariance: \(H(A)_\rho = H(A)_{U\rho U^\dagger}\).</li>
<li>Positivity: \(H(A)_\rho \geq 0\).</li>
<li>Additivity for uncorrelated states: \(H(AB)_{\rho_A\otimes\rho_B} = H(A)_{\rho_A} + H(B)_{\rho_B}\).</li>
<li>For \(\rho_{AB}\) a quantum-classical state \(\rho_{AB} = \sum_b \Pr(B=b) \rho_{A,b} \ketbra b\) then \(H(AB)_{\rho_{AB}} = H(\Pr(B=b)) + \sum_b \Pr(B=b) H(A)_{\rho_{A,b}})\).</li>
</ul>

</div>

<p>
and specifically for the von Neumann entropy
</p>
<div class="property" id="org3a768ef">
<p>
\(H(A)_\rho \leq \log_2 d\), where \(d\) is the dimension of the Hilbert space of \(A\).
</p>

</div>
</div>
</div>

<div id="outline-container-orgad9cf59" class="outline-3">
<h3 id="orgad9cf59"><span class="section-number-3">5.2.</span> Conditional entropy</h3>
<div class="outline-text-3" id="text-5-2">
<p>
There could a priori be several ways of defining conditional von Neumann entropy for a density matrix \(\rho_{AB}\) with respect to the \(A\), \(B\) cut. The most tempting would be to define it through a measurement on \(B\) that gives a different reduced density matrix on \(A\) for each of the possible outcomes &#x2014; this would be reminiscent of the computation for quantum-classical states above. The problem is that such conditional entropy would involve some optimization over the possible measurements and would not necessarily satisfy some of the other properties expected from conditional entropy. In fact the right way to define the conditional entropy is through the classical identity \(H(X|Y) = H(X,Y) - H(Y)\).
</p>

<div text="Conditional Entropy" class="definition" id="orgfa92034">
<p>
The quantum conditional entropy for a state \(\rho_{AB}\) for subsystems \(A\), \(B\) is
</p>
\begin{equation}
H(A|B)_{\rho_{AB}} = H(AB)_{\rho_{AB}} - H(B)_{\rho_{AB}},
\end{equation}
<p>
where \(H(B)_{\rho_{AB}} = H(B)_{\rho_B}\) for \(\rho_B = \tr_A \rho_{AB}\).
</p>

</div>

<div text="Chain Rule" class="property" id="org87bedb0">
<p>
Reversing the definition of \(H(A|B)\) we obtain the chain rule for quantum entropies:
</p>
\begin{equation}
H(AB)_\rho = H(A|B)_\rho + H(B)_\rho.
\end{equation}

</div>

<div class="remark" id="org91cd62a">
<p>
Whereas conditional entropy of random variables is positive, it is not necessary the case for quantum conditional entropy. E.g. take a pure bi-partite state \(\rho\) over \(A\) and \(B\), then \(H(AB)_\rho = 0\) and \(H(A)_\rho = H(B)_\rho \geq 0\), which implies \(H(A|B)_\rho = H(B|A)_\rho \leq 0\).
</p>

</div>

<div class="property" id="org7244068">
<p>
For a quantum-classical state \(\rho_{AB} = \sum_b p(b) \rho_{A,b}\otimes \ketbra b\) over \(A\) and \(B\), we have
</p>
\begin{equation}
H(A|B)_{\rho_{AB}} = \sum_b p(b) H(A)_{\rho_{A,b}},
\end{equation}
<p>
and consequently, \(H(A|B) \geq 0\).
</p>

</div>

<div text="Strong subadditivity" class="property" id="org9f380d8">
<p>
For \(\rho_{ABC}\) a state for the 3 subsystems \(A\), \(B\) and \(C\), then
</p>
\begin{equation}
H(A|B)_{\rho_{ABC}} \geq H(A|BC)_{\rho_{ABC}}.
\end{equation}

</div>

<p>
This inequality is fundamental in quantum theory as many other properties derive from it. We will see one consequence in the next paragraph.
</p>
</div>
</div>

<div id="outline-container-org697bdc6" class="outline-3">
<h3 id="org697bdc6"><span class="section-number-3">5.3.</span> Mutual information</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Mutual information, once conditional entropy is defined, follows from the same definition as in the classical case:
</p>
<div text="Quantum mutual information" class="definition" id="org25ca7be">
<p>
For a state \(\rho\) over subsystems \(A\) and \(B\), the quantum mutual information is defined as:
</p>
\begin{equation}
I(A:B)_\rho = H(A)_\rho - H(A|B)_\rho = H(B)_\rho - H(B|A)_\rho = H(A)_\rho + H(B)_\rho - H(AB)_\rho.
\end{equation}

</div>

<div class="property" id="orgf8d2746">
\begin{equation}
I(A:B)_\rho \geq 0.
\end{equation}

</div>

<div class="proof" id="org2a90b04">
<p>
Apply the strong subadditivity for \(H(X|Y)_{\rho_{XYZ}}\) with \(\rho_{XYZ} = \rho_{XZ} \otimes \ketbra 0_Y\) taking \(X=A\), \(Y=B\) and \(Z=C\). Note that the general case (not setting up \(Y\) in state \(\ketbra 0\) gives that the conditional mutual information is positive.
</p>

</div>

<p>
Another interesting property of quantum mutual information states that quantum post processing subsystem \(B\) does not increase the quantum mutual information with \(A\). More precisely:
</p>
<div class="property" id="orgb257d78">
<p>
Let three subsystems \(A\), \(B\), \(C\), \(\rho_{AB}\) a state over \(AB\) and \(\mathcal E\) a CPTP map from \(B\) to \(C\). Then:
</p>
\begin{equation}
I(A:B)_{\rho_{AB}} \geq I(A:C)_{(\one_A\otimes \mathcal E)(\rho_{AB})}.
\end{equation}

</div>

<div class="proof" id="orgb5e54f3">
<p>
We prove that \(H(A|B)_{\rho_{AB}} \leq H(A|C)_{(\one_A\otimes \mathcal E)(\rho_{AB})}\). This is a direct application of the fact that \(\mathcal E\) can be implemented by considering an auxiliary quantum system \(D\) and a joint isometry from \(BD\) to \(CD\), and the strong subadditivity.
</p>

</div>
</div>
</div>
</section>

<section id="outline-container-org017c8b4" class="outline-2">
<h2 id="org017c8b4"><span class="section-number-2">6.</span> References</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li><a href="https://edu.itp.phys.ethz.ch/hs15/QIT/renner_lecture_notes12.pdf">Quantum Information Theory, R. Renner (2013)</a></li>
<li><a href="https://arxiv.org/pdf/1106.1445.pdf">From Classical to Quantum Shannon Theory, M. Wilde (2019)</a></li>
</ul>
</div>
</section>
</article>]]></content><author><name></name></author><category term="Lecture" /><summary type="html"><![CDATA[Foundations and tools In this lecture, we introduce the foundational questions of information theory, both in the classical and quantum setting. This serves to emphasize the fact that quantum information theory is really information theory specialized to information carriers that abide by the axioms of quantum mechanics, and as a motivation to develop a set of tools that are useful to tackle more complex tasks.]]></summary></entry><entry><title type="html">Open Quantum Systems</title><link href="http://localhost:4000/2023/09/19/open_quantum_systems.html" rel="alternate" type="text/html" title="Open Quantum Systems" /><published>2023-09-19T00:00:00+02:00</published><updated>2023-09-19T00:00:00+02:00</updated><id>http://localhost:4000/2023/09/19/open_quantum_systems</id><content type="html" xml:base="http://localhost:4000/2023/09/19/open_quantum_systems.html"><![CDATA[<article id="content" class="content">
<section id="outline-container-org39ca655" class="outline-2">
<h2 id="org39ca655"><span class="section-number-2">1.</span> The Church of the larger Hilbert space</h2>
<div class="outline-text-2" id="text-1">
<p>
The axioms of quantum mechanics presented earlier only refer to <i>closed quantum systems</i>. So it is only possible to describe what happens to a system completely isolated from other systems. This seems like a restriction, until you realize that indeed, it is always possible to view an <i>open quantum system</i> as a subsystem of a larger quantum system, which is itself isolated. In case of doubt, you can always throw in the full wave function of the universe! This lecture is about exploiting this ability to better understand states of open quantum systems, their dynamics and the allowed measurements. The term "Church of the larger Hilbert space" refers to this ability to enlarge the Hilbert space by adding more and sometimes fictitious subsystems in order to recover pure states and closed systems dynamics and measurements.
</p>
</div>
</section>

<section id="outline-container-org2546e71" class="outline-2">
<h2 id="org2546e71"><span class="section-number-2">2.</span> From pure to mixed states and back</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org282751f" class="outline-3">
<h3 id="org282751f"><span class="section-number-3">2.1.</span> A Tool for obtaining mixed states from pure ones</h3>
<div class="outline-text-3" id="text-2-1">
<div text="Partial trace" class="definition" id="org6aba917">
<p>
The <i>partial trace</i> is a mapping from \(End(\mathcal H_A \otimes \mathcal H_B)\) to \(End(\mathcal H_B)\). It is defined as the linear extension of the following:
\[O_A \otimes O_B \rightarrow \tr_A(O_A \otimes O_B) \coloneqq \tr(O_A) O_B.\]
</p>

</div>

<p>
Using that for an operator in \(End(\mathcal H_A)\otimes End{\mathcal H_B}\) , \(O = \sum_{i,j,k,l} O_{ij,kl}\ketbra{i}{j}_A\otimes \ketbra{k}{l}_B\), for \(\{\ket i_A\}_i\) and \(\{\ket{k}_B\}_B\) bases of \(\mathcal H_A\) and \(\mathcal H_B\), we obtain:
</p>
\begin{align}
\tr_A(O) &amp; = \sum_{i,k,l} O_{ii,kl}\ketbra{k}{l}_B \\
&amp; = \sum_{i,k,l} (\bra i_A \otimes \bra k_B) O (\ket i_A \otimes \ket l_B) \ketbra{k}{l}_B\\
&amp; = \sum_{i} (\bra i \otimes \one_B) O (\ket i \otimes \one_B),
\end{align}
<p>
which is often abreviated by omitting the \(\one_B\) factor:
</p>
\begin{equation}
\tr_A(O) = \sum_i \bra i_A O \ket i_A.
\end{equation}

<div text="Marginals and Partial Trace (I)" class="remark" id="org9facce5">
<p>
The partial trace for classical composite states corresponds to marginalization of classical joint probability distributions:
Let \(\rho_{X,Y} = \sum_{x,y} \Pr(X=x, Y=y) \ketbra x_X \otimes \ketbra y_Y\), for \(\{\ket x_X\}\) and \(\{\ket y_Y\}\) two orthonormal bases for respectively a system \(X\) and a system \(Y\), then
</p>
\begin{align}
\tr_X (\rho_{X,Y})
&amp; = \sum_{x'} \sum_{x,y} \Pr(X=x, Y=y) \bra {x'}( \ketbra x) \ket{x'} \ketbra y \\
&amp; = \sum_y \Pr(Y = y) \ketbra y,
\end{align}
<p>
where \(\Pr(Y=y) = \sum_x Pr(X=x, Y=y)\).
</p>

</div>

<p>
A similar understanding can be build for generic quantum states: 
</p>
<div text="Marginals and Partial Trace (II)" class="remark" id="org06ea418">
<p>
The partial trace corresponds to measuring part of  a composite quantum system using a PVM and being struck by amnesia right after<label id="fnr.1" for="fnr-in.1.1088902" class="margin-toggle sidenote-number"><sup class="numeral">1</sup></label><input type="checkbox" id="fnr-in.1.1088902" class="margin-toggle" /><span class="sidenote"><sup class="numeral">1</sup>This can be generalized to POVM measurements which will be introduced below.</span>. Let \(\rho\) be a density matrix on \(\mathcal H_A \otimes \mathcal H_B\), then, for \(\{M_i \}\) a PVM on \(A\), we can define the set of quantum vectors  \(\{\ket {e_{i,j}}\}_{i,j}\) such that for a given \(i\), \(\{\ket {e_{i,j}}\}_{j}\) is an orthonormal basis of the subspace stabilized by \(M_i\). Then \(\{\ket {e_{i,j}}\}_{i,j}\) is an orthonormal basis of \(\mathcal H_A\) and \(M_i = \sum_{j} \ketbra{e_i{i,j}}\).<label id="fnr.2" for="fnr-in.2.2562339" class="margin-toggle sidenote-number"><sup class="numeral">2</sup></label><input type="checkbox" id="fnr-in.2.2562339" class="margin-toggle" /><span class="sidenote"><sup class="numeral">2</sup>Note that here \(\ket{e_{i,j}}\) are vectors on \(\mathcal H_A\), but we omit the index \(A\) to lighten the notation.</span> Hence, 
</p>
\begin{align}
\tr_A(\rho) &amp; = \sum_{i,j} (\bra{e_{i,j}} \otimes \one_B) \rho (\ket{e_{i,j}} \otimes \one_B) \\
&amp; = \sum_{i} \sum_{j} \bra{e_{i,j}} (M_i \otimes \one_B) \rho (M_i \otimes \one_B)\ket{e_{i,j}} \\
&amp; = \sum_{i} \sum_{j} \bra{e_{i,j}} (M_i \otimes \one_B) \rho (M_i \otimes \one_B)\ket{e_{i,j}} \\
&amp; = \sum_{i} \tr_A \left((M_i \otimes \one_B) \rho  (M_i \otimes \one_B) \right)\\
&amp; = \sum_{i} p_i \frac{\tr_A \left((M_i \otimes \one_B) \rho  (M_i \otimes \one_B)\right)}{\tr \left((M_i \otimes \one_B) \rho  (M_i \otimes \one_B)\right)} \\
\end{align}
<p>
with \(p_i = \tr \left((M_i \otimes \one_B) \rho  (M_i \otimes \one_B)\right)\) is the probability of obtaining outcome \(i\) when performing the PVM \(\{M_i\}\). Hence the partial trace corresponds to the average resulting state of subsystem \(B\) after the measurement, i.e. \(A\) is measured and the outcome is forgotten. Note that the invariance of the partial trace under unitary transformations implied that the resulting state does not depend on which measurement has been chosen. That is any measurement does the job equally well, even the measurement that consist of returning always the same outcome.
</p>

</div>

<p>
In particular, if we look at a composite system \(A-B\) in a global pure state, then we see that tracing out \(A\) will in general yield a mixed state. Let \(\ket \psi \in \mathcal H_A \otimes \mathcal H_B\). The most general way to write \(\ket \psi\) given two orthonormal bases \(\{\ket i_A\}\) and \(\{\ket j_B\}\) of \(\mathcal H_A\) and \(\mathcal H_B\), is
</p>
\begin{equation}
\ket\psi = \sum_{i,j} \psi_{i,j} \ket i_A\otimes \ket j_B.
\end{equation}
<p>
Taking the partial trace we obtain:
</p>
\begin{align}
\tr_A \ketbra{\psi} &amp; = \sum_m \bra m_A \left(\sum_{i,j,k,l} \psi_{i,k}\psi^*_{j,l} \ketbra{i}{j}_A\otimes\ketbra{k}{l}_B \right) \ket m_A \\
&amp; = \sum_{i,k,l} \psi_{i,k}\psi^*_{i,l} \ketbra{k}{l}_B.
\end{align}
<p>
We see that in general this does not correspond to a pure state \(\ketbra \psi\). One way to produce a mixed state on \(B\) is thus to consider a larger system \(A-B\) and to trace out \(A\). The natural question then is to what extent any mixed state \(\rho_B\) on \(B\) can be produced in this way, and whether there is or not a single way to do this.
</p>
</div>
</div>

<div id="outline-container-orgb87ad8a" class="outline-3">
<h3 id="orgb87ad8a"><span class="section-number-3">2.2.</span> From mixed states to pure ones</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Here we try to answer the question: given a mixted state \(\rho_B\) on \(\mathcal B\), is it always possible to view it as the partial trace of a larger system \(A-B\)? We will answer positively and constructively to this question.
</p>

<p>
First, recall that we introduced density matrices through the idea of preparations, i.e. we imagined an experimentalist \(A\) preparing a state \(\ket \psi_i\) with probability \(p_i\) and sending it to \(B\) without telling him the value of \(i\). This resulted in \(\rho_B = \sum p_i \ketbra {\psi_i}\). With the notion of partial trace seen above, it is easy to find a pure state \(\ket \psi\) on \(\mathcal H_A \otimes \mathcal H_B\)  that will produce precisely \(\ket \psi_i\) with probability \(p_i\) under an appropriate measurement on \(A\):
</p>
<div text="Purification of a Preparation" class="definition" id="org7532ec0">
<p>
Given a preparation \(\{\ket {\psi_i}_B, p_i \}_i\) of a system \(B\), the puritification of the preparation is the quantum state
</p>
\begin{equation}
\ket \psi = \sum_i \sqrt{p_i} \ket i_A \otimes \ket {\psi_i}_B,
\end{equation}
<p>
Where \(\{\ket i_A\}\) is an orthonormal basis of \(\mathcal H_A\) a sufficiently large Hilbert space of an ancillary system \(A\). 
</p>

</div>

<p>
Now, starting from a generic \(\rho_B\) positive semi-definite trace 1 matrix, we can use that fact that there is a canonical preparation of \(\rho_B\). Indeed, \(\rho_B\) is hermitian and can thus be diagonalized: \(\rho_B = \sum \lambda_i \ketbra{i}_B\). Hence, we can purify this preparation and obtain:
</p>
\begin{align}
&amp; \rho_B = \tr_A \ketbra \psi, \mbox{ with} \\
&amp; \ket \psi = \sum_i \sqrt{\lambda_i} \ket i_A \otimes \ket i_B
\end{align}
<p>
where \(\ket i_A\) is an orthonormal basis for an ancillary system \(A\). Note that one can always perform this purification using a system \(A\) with dimension equal to that of \(B\). Such purification is usually called "the" purification of \(\rho_B\) even though it is obviously not unique as one can perform basis changes on \(A\) without affecting the obtained state.<label id="fnr.3" for="fnr-in.3.5952836" class="margin-toggle sidenote-number"><sup class="numeral">3</sup></label><input type="checkbox" id="fnr-in.3.5952836" class="margin-toggle" /><span class="sidenote"><sup class="numeral">3</sup>Note that we cannot say all purifications are unitarily equivalent.</span>
</p>

<p>
In conclusion, pure states on composite system give density matrices when one part is traced out. Conversely, any valid density matrix can be associated to a pure state of a composite system. This is a very useful property to understand static situations, but in fact we woud be interested to understand the dynamics of quantum systems in this way. In other terms, when \(\rho\) evolves is there a corresponding evolution for the purification of \(\rho\)?
</p>
</div>
</div>
</section>

<section id="outline-container-org0785af1" class="outline-2">
<h2 id="org0785af1"><span class="section-number-2">3.</span> Evolutions</h2>
<div class="outline-text-2" id="text-3">
<p>
The topic of this subsection is to understand what are the possible evolutions for a system undergoing an open evolution starting from  \(\mathcal H\) and ending in \(\mathcal H'\). First we will seek to determine the necessary conditions imposed by quantum mechanics on transformations from \(\mathcal H\) to \(\mathcal H'\). Then we will look at sufficiency. Along the way, we will indeed see that going to the Church of the Larger Hilbert space is indeed a very powerful concept.
</p>
</div>

<div id="outline-container-orgdf771d8" class="outline-3">
<h3 id="orgdf771d8"><span class="section-number-3">3.1.</span> Choi-Jamiolkowski Isomorphism</h3>
<div class="outline-text-3" id="text-3-1">
<div text="Choi-Jamiolkowski Isomorphism" class="definition" id="org042c877">
<p>
Let \(\mathcal H_A\) and \(\mathcal H_B\) be two finite dimensional Hilbert-spaces and consider \(\mathcal H_{A'}\)  isomorphic to \(\mathcal H_A\). Consider \(\Gamma \in \mathcal H_{A'}\otimes \mathcal H_{A}\)
</p>
\begin{equation}
\Gamma = \frac{1}{\sqrt d} \sum_{i = 1}^{d} \ket{e_i}_A' \otimes \ket{e_i}_A, 
\end{equation}
<p>
for \(d = \dim(\mathcal H_A)\) and \(\{\ket{e_i}_A\}_i\) a basis of \(\mathcal H_A\) and \(\{\ket{e_i}_{A'}\}_i\) a basis of \(\mathcal H_{A'}\).<label id="fnr.4" for="fnr-in.4.6434766" class="margin-toggle sidenote-number"><sup class="numeral">4</sup></label><input type="checkbox" id="fnr-in.4.6434766" class="margin-toggle" /><span class="sidenote"><sup class="numeral">4</sup>Because \(\mathcal H_A \simeq \mathcal H_{A'}\) the two bases have the same number of vectors.</span> The Choi-Jamiolkowski Isomorphism relative to the bases \(\{\ket{e_i}_A\}_i\) and  \(\{\ket{e_i}_{A'}\}_i\) is the linear map:
</p>
\begin{align}
\mathsf{C}\ : \
&amp;  Hom(End(\mathcal H_A),End(\mathcal H_B)) \rightarrow End(\mathcal H_{A'} \otimes \mathcal H_B) \\
&amp; \mathcal E \mapsto (\one_{A'} \otimes \mathcal E) [\ketbra{\Gamma}] 
\end{align}

</div>

<div class="lemma" id="orgee1bd82">
<p>
The Choi-Jamiolkowski Isomorphism is an isomorphism!
</p>

</div>

<div class="proof" id="orga769bc8">
<p>
Because \(\mathsf{C}\) is linear, we simply have to show that a basis of \(Hom(End(\mathcal H_A),End(\mathcal H_B))\) is transformed into a basis of \(End(\mathcal H_{A'} \otimes \mathcal H_B)\).
</p>

<p>
Indeed, using the basis \(\{\ket{e_i}_A\}_i\) for \(\mathcal H_A\) and \(\{\ket{f_k}_B\}_B\) for \(\mathcal H_B\), a basis of \(End(\mathcal H_A)\) is given by \(\{\ketbra{e_i}{e_j}_A\}_{i,j}\), while \(\{\ketbra{f_k}{f_l}_B \}_{k,l}\) is a basis for \(End(\mathcal H_B)\). A basis for \(Hom(End(\mathcal H_A),End(\mathcal H_B))\) can thereby be constructed by mapping one basis vector of \(End(\mathcal H_A)\) to one basis vector of \(End(\mathcal H_B)\) and mapping everything else to \(0\). That is,
</p>
\begin{equation}
\left \{ \mathcal E_{ij,kl}\ : \mathcal E_{ij,kl} [\ketbra{e_i}{e_j}_A] = \ketbra{f_k}{f_l}_B \mbox{ and 0 elsewhere} \right \}_{ij,kl}
\end{equation}
<p>
is a basis of \(Hom(End(\mathcal H_A),End(\mathcal H_B))\).
</p>

<p>
We then need to compute \((\one_{A'} \otimes \mathcal E_{ij,kl})[\ketbra\Gamma]\):
</p>
\begin{align}
(\one_{A'} \otimes \mathcal E_{ij,kl})[\ketbra{\Gamma}]
&amp; = (\one_{A'} \otimes \mathcal E_{ij,kl})[\frac{1}{d}\sum_{mn}\ketbra{e_m}{e_n}_{A'}\otimes \ketbra{e_m}{e_n}_A] \\
&amp; = \frac{1}{d}\ketbra{e_i}{e_j}_{A'}\otimes \ketbra{f_k}{f_l}_B
\end{align}
<p>
which generates a basis of  \(End(\mathcal H_{A'} \otimes \mathcal H_B)\) when \(i,j,k,l\) are varied. So we just prooved that \(\mathsf C\) is sendind a basis to a (non normalized) basis so it is an isomorphism.
</p>

</div>

<div class="remark" id="org9271c19">
<p>
Seeing \(End(\mathcal H_A)\) as a vector space with a basis \(\ket{\ket{i}}_A\) and similarly for \(End(\mathcal H_B)\), the Choi-Jamiolkowski isomorphism is the mapping \(\ket{\ket{j}}_B \ket{\ket{i}}_A \mapsto \ket{\ket{i}}_A \otimes \ket{\ket j}_B\) up to the isomorphism between \(\mathcal H_A\) and \(\mathcal H_{A'}\).
</p>

</div>
</div>
</div>

<div id="outline-container-orge9d405e" class="outline-3">
<h3 id="orge9d405e"><span class="section-number-3">3.2.</span> Necessary conditions</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Equipped with the Choi-Jamiolkowski isomorphism between operators on endomorphisms and endomorphisms on an enlarged Hilbert space, we can now come back to the original motivation of understanding transformations of open quantum systems, i.e. of density matrices viewed as subsets of endomorphisms.
</p>

<p>
First, we realize that for any preparation of \(\rho\) as \(\{(\rho_1, p_1), (\rho_2, p_2)\}\) then we should have
</p>
\begin{align}
\mathcal E[\rho] &amp; = \mathcal E[p_1 \rho_1 + p_2 \rho_2] \\
&amp; = p_1 \mathcal E[\rho_1] + p_2 \mathcal E[\rho_2].
\end{align}
<p>
This implies that \(\mathcal E\) is linear but also that \(\mathcal E[\rho]\) is a positive semi-definite operator with unit trace.
</p>

<div text="Positivity of Operators" class="definition" id="org22d8f56">
<p>
\(\mathcal E \in Hom(End(\mathcal H_A), End(\mathcal H_B)\) is <i>positive</i> if
</p>
\begin{equation}
\forall \rho \geq 0, \ \mathcal E[\rho] \geq 0.
\end{equation}

</div>

<p>
But if \(\mathcal E\) is a physical operator, then so should be \(\mathcal E \otimes \one_C\) for a subsystem \(C\) with Hilbert space \(\mathcal H_C\) that is unaffected by the evolution \(\mathcal E\), but which could exhibit initial correlations with the subsystem \(A\). Hence, not only should we have the positivity of \(\mathcal E\), but we should have the positivity of \(\mathcal E \otimes \one_C\) for any \(C\). 
</p>

<div text="Complete Positivity of Operators" class="definition" id="org51e3234">
<p>
\(\mathcal E \in Hom(End(\mathcal H_A), End(\mathcal H_B)\) is <i>completely positive</i> if
</p>
\begin{equation}
\forall \mathcal H_C, \ \forall \rho \in End(\mathcal A \otimes \mathcal C), \ \rho  \geq 0, \ (\mathcal E \otimes \one_C)[\rho] \geq 0.
\end{equation}

</div>

<p>
Finally, we define
</p>
<div text="Trace Preserving Operators" class="definition" id="org15983a0">
<p>
\(\mathcal E \in Hom(End(\mathcal H_A), End(\mathcal H_B)\) is <i>trace preserving</i> if \(\tr(\mathcal E[\rho]) = \tr(\rho)\).
</p>

</div>

<p>
Hence, to summarize, quantum maps need to be:
</p>
<ul class="org-ul">
<li>linear maps from \(End(\mathcal H_A)\) to \(End(\mathcal H_B)\)</li>
<li>completely positive</li>
<li>trace preserving.</li>
</ul>
<p>
Such maps are called <i>Completely Positive Trace Preserving (CPTP)</i> maps.
</p>
</div>
</div>
<div id="outline-container-org9252f34" class="outline-3">
<h3 id="org9252f34"><span class="section-number-3">3.3.</span> Quantum evolution as quantum state (Choi state)</h3>
<div class="outline-text-3" id="text-3-3">
<div text="Choi state" class="definition" id="orge3e5da1">
<p>
For \(\mathcal E\) a quantum evolution from system \(A\) to system \(B\), given \(A'\) such that \(\mathcal H_A \simeq \mathcal H_{A'}\),
</p>
\begin{equation}
(\one_{A'} \otimes \mathcal E) [\ketbra \Gamma], 
\end{equation}
<p>
where \(\ket \Gamma_{A'A} = \frac{1}{\sqrt d} \sum_i \ket i_{A'} \otimes \ket i_A\) is the <i>Choi-state</i> of \(\mathcal E\). 
</p>

</div>

<div class="remark" id="org7b8c4c4">
<p>
The Choi-state of \(\mathcal E\) contains all the information about \(\mathcal E\) as per the Choi-Jamiolkowski isomorphism.
</p>

</div>

<div class="remark" id="org23d0f41">
<p>
If \(\mathcal E\) is a quantum evolution, then the Choi-state is a regular quantum state. It can be prepared in the lab:
</p>
<ol class="org-ol">
<li>Prepare a maximally entangled state;</li>
<li>Send half of it through \(\mathcal E\) while keeping the other one as a reference.</li>
</ol>
<p>
As a consequence, the Choi-state is a positive semi-definite trace 1 matrix. We will use this as a crucial ingredient to show that indeed all CPTP maps are legitimate evolutions for open quantum systems. 
</p>

</div>

<div class="remark" id="orge56b36b">
<p>
Because of the previous remarks, we note that to compare two channels, one can perform the preparation of Choi-states for both an perform tomography on them. If we are only interested in knowing if the two channels are equal or not, a SWAP-test can be performed instead.
</p>

</div>
</div>
</div>

<div id="outline-container-orge5119f2" class="outline-3">
<h3 id="orge5119f2"><span class="section-number-3">3.4.</span> Sufficient conditions</h3>
<div class="outline-text-3" id="text-3-4">
<p>
Let \(\mathcal E \in Hom(End(\mathcal H_A), End(\mathcal H_B))\) that satisfies the 3 necessary conditions above. We will show that it is always possible to consider such transformation as a unitary transformation on a larger Hilbert space, so that these conditions are indeed sufficient.
</p>
</div>

<div id="outline-container-orgd12c0a1" class="outline-4">
<h4 id="orgd12c0a1"><span class="section-number-4">3.4.1.</span> Operator sum representation</h4>
<div class="outline-text-4" id="text-3-4-1">
<p>
To show that the CPTP condition is indeed sufficient, we start by constructing the operator sum representation of quantum evolutions. To this end, consider the Choi state \(\rho_{\mathcal E}\)  associated to a CPTP map \(\mathcal E\). Because \(\mathcal E\) is CPTP, the Choi-state is a positive semi-definite trace 1 matrix, i.e. it is an allowed quantum state. As a consequence, \(\rho_{\mathcal E}\) can be diagonalized:
</p>
\begin{equation}
\rho_{\mathcal E} = \sum_{k \in [1, d]} \ketbra{\Gamma^k}_{A'B},
\end{equation}
<p>
where \(d \leq d_A d_B\) and \(\ket{\Gamma^k}\) is unnormalized. Now we can expand \(\ket {\Gamma^k}_{A'B}\) over a basis of \(\mathcal H_{A'} \otimes \mathcal H_B\):
</p>
\begin{equation}
\ket{\Gamma^k}_{A'B} = \sum_{i,j} \alpha^k_{i,j} \ket i_{A'} \otimes \ket j_B,
\end{equation}
<p>
and construct operators \(V^k\) on \(Hom(End(\mathcal H_A),End(\mathcal H_B))\) defined by:
</p>
\begin{equation}
V^k = \sum_{i,j} \alpha^k_{i,j} \ket{j}_B \bra{i}_A.
\end{equation}
<p>
The definition of these \(V^k\) is such that we have \((\one_{A'} \otimes V^k) \ket{\Gamma}_{A'A} = \ket{\Gamma^k}_{A'B}\).
Combining this with the definition of \(\rho_{\mathcal E}\) we obtain:
</p>
\begin{equation}
\rho_{\mathcal E} = \sum_k (\one_{A'} \otimes V^k ) \ketbra{\Gamma}_{A'A} (\one_{A'} \otimes V^{k\dagger}),
\end{equation}
<p>
which gives \(\mathcal E[\rho_A] = \sum_k V^k \rho_A V^{k\dagger}\) using \(\mathcal E[\ketbra{i}{j}_A] = (\bra i_{A'}\otimes \one_B  )\rho_{\mathcal E}(\ket j_{A'} \otimes \one_B)\).
</p>

<p>
Additionally, because \(\mathcal E\) is trace preserving, \(\tr(\mathcal E[{\ketbra{i}{j}}_A]) = \delta_{i,j}\). But
</p>
\begin{align}
\tr(\mathcal E[{\ketbra{i}{j}}_A]) &amp; = \tr\left (\sum_k V^k{\ketbra{i}{j}}_A V^{k\dagger} \right) \\
    &amp; = \bra j_A \left(\sum_k{V^{k\dagger} V^k}\right) \ket i_A,
\end{align}
<p>
so that we have \(\sum_k V^{k\dagger}V^k = \one_A\).
</p>

<div text="Kraus operators" class="definition" id="orgadd9f1e">
<p>
The operators \(V^k\) are called <i>Kraus operators</i>. They satisfy:
</p>
<ul class="org-ul">
<li>\(V^k \in Hom(\mathcal H_A, \mathcal H_B)\),</li>
<li>\(\sum_{k\in [1,d]} V^{k\dagger}V^k = \one_A\)</li>
</ul>

</div>

<p>
As consequence of the above remark, we see that:
</p>
<div text="Operator Sum Representation" class="definition" id="orgdaaa402">
<p>
For any CPTP map \(\mathcal E\), there exists Kraus operators such that:
</p>
\begin{equation}
\mathcal E[\rho] = \sum_k V^k \rho V^{k\dagger}
\end{equation}

</div>

<div class="remark" id="orge7ba525">
<p>
Any set of Kraus operators defines an admissible CPTP map. This is because:
</p>
<ol class="org-ol">
<li>The map defined by \(\sum_k V^k \rho V^{k\dagger}\) is linear;</li>
<li>Is completely positive<label id="fnr.5" for="fnr-in.5.3217473" class="margin-toggle sidenote-number"><sup class="numeral">5</sup></label><input type="checkbox" id="fnr-in.5.3217473" class="margin-toggle" /><span class="sidenote"><sup class="numeral">5</sup>\(\langle \psi, (\one_{A'} \otimes V^k) \rho_{A'A} (\one_{A'}\otimes V^{k\dagger}) \psi \rangle = \langle \psi (\one_{A'} \otimes V^{k\dagger}),  \rho_{A'A} (\one_{A'}\otimes V^{k\dagger}) \psi \rangle \geq 0\) using \(\psi' = (\one_{A'}\otimes V^{k\dagger}) \psi\) and the positivity of \(\rho\).</span> as \((\one_{A'} \otimes V^k) \rho_{A'A} (\one_{A'}\otimes V^{k\dagger})\) is PSD for any \(k\) and any PSD \(\rho\)</li>
</ol>

</div>

<p>
As a round up remak, we have used Choi-Jamiolkowsi isomorphism in the forward direction to transport our requirements onto quantum evolutions &#x2014; i.e. complete positivity and trace preservation &#x2014; to necessary properties of the Choi state. From there, using vector spaces tools, we derived a generic form of operators defined by positive semi-definite trace 1 Choi-states. This gave the operator sum representation. And we then checked that any such operator was CPTP. Hence, we are half way to our original motivation: we have seen how to represent any CPTP map as Kraus operators. Yet, we are still missing the answer to why all of these CPTP maps are legitimate quantum operaations. The answer comes in the form of the Stinespring dilation theorem.
</p>
</div>
</div>

<div id="outline-container-org245a479" class="outline-4">
<h4 id="org245a479"><span class="section-number-4">3.4.2.</span> Stinespring dilation</h4>
<div class="outline-text-4" id="text-3-4-2">
<p>
To conclude on the sufficiency of the CPTP criterion above, we show how such maps can be constructed from a closed system evolution. To this end, consider a set of Kraus operators \(\{V^k\}_k\) implementing a CPTP map \(\mathcal E\). Then construct the map:
</p>
\begin{equation}
U = \sum_k V^k \otimes \ket k_C
\end{equation}
<p>
where \(C\) is a fiducial quantum system with sufficiently many dimensions to be able to pair each \(V^k\) with a different basis vector \(\ket k\). \(U\) is a map from \(\mathcal H_A\) to \(\mathcal B \otimes \mathcal C\) that satisfies
</p>
\begin{align}
U^\dagger U &amp; = \left( \sum_k V^{k\dagger}\otimes \bra k \right ) \left( \sum_l V^l \otimes \ket l\right ) \\
&amp; = sum_{kl} V^{k\dagger} V^l \braket{k,l} \\
&amp; = \one_A. 
\end{align}
<p>
Hence \(U\) is an isometry and can be seen as a closed quantum system evolution<label id="fnr.6" for="fnr-in.6.2637067" class="margin-toggle sidenote-number"><sup class="numeral">6</sup></label><input type="checkbox" id="fnr-in.6.2637067" class="margin-toggle" /><span class="sidenote"><sup class="numeral">6</sup>To this end, we just need to embed this evolution from \(\mathcal H_A\) to \(\mathcal H_B \otimes \mathcal H_C\) into endomorphisms of \(\mathcal H_A \otimes \mathcal H_B \otimes \mathcal H_C\) and constructing a unitary that coincides with \(U\) when a given fiducial state \(\ket 0_B \ket 0_C\) is used as inputs of \(B\) and \(C\).</span>
</p>

<p>
Now, to conclude, we simply realize that
</p>
\begin{align}
U \rho_A U^\dagger &amp; = \sum_{kl} (V^k\otimes \ket k_C) \rho_A(V^{l\dagger}\otimes l_C) \\
&amp; = \sum_{kl} (V^k \rho_A V^{l\dagger}) \otimes \ketbra{k}{l}_C,
\end{align}
<p>
so that \(\tr_C(U\rho U^\dagger) = \mathcal E[\rho]\).
</p>

<p>
Hence, we arrive at the final conclusion: any CPTP map can be seen as unitary evolution on a larger Hilbert space. This is known as <i>Stinespring dilation theorem</i>. And it also shows that, because all unitary transformtations are legitimate quantum operations, any CPTP map is a legal quantum evolution permitted by the axioms of closed systems quantum mechanics.
</p>
</div>
</div>
</div>
</section>
<section id="outline-container-org303053a" class="outline-2">
<h2 id="org303053a"><span class="section-number-2">4.</span> Measurements</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org4506412" class="outline-3">
<h3 id="org4506412"><span class="section-number-3">4.1.</span> (Detour) Post measurement state for PVM</h3>
<div class="outline-text-3" id="text-4-1">
<p>
We intoduced PVM as a collection of rank-1 projectors \(\{\Pi_i\}_i\) on the Hilbert space of the measured system \(\mathcal H\) such that they resolve the identity &#x2014; i.e. \(\sum_i \Pi_i = \one_{\mathcal H}\). For a system in state \(\rho\), the measurement outcome \(i\) associated to \(\Pi_i\) is obtained with probability \(\Pr_{\rho} (i) = \tr(\Pi_i\rho)\).
</p>

<p>
Yet, we haven't said anything about the post-measurement state. In fact, there is a variety of models for the measurements so that I chose <i>not</i> to include it in the main axioms of quantum theory. Yet, the question "what is the post-measurement state of the measured quantum system?" is legitimate. To answer it, we will nonetheless postulate that for what we will call <i>non destructive (and minimally disturbing) measurement</i> if one performs the same measurement twice in a row, without the system being able to evolve between the two rounds of measurements, then the results shoud be the same. That is, if we denote by \(\rho_i\) the post measurement state after outcome \(i\) is obtained, we should have:
\[\tr(\Pi_j \rho_i) = \delta_{i,j}.\]
Using the positivity of \(\rho_i\) and the fact that \(\tr(\rho_i)=1\), we can conclude that \(\rho_i = \Pi_i\). Hence, we can summarize this into the following formula<label id="fnr.7" for="fnr-in.7.7392082" class="margin-toggle sidenote-number"><sup class="numeral">7</sup></label><input type="checkbox" id="fnr-in.7.7392082" class="margin-toggle" /><span class="sidenote"><sup class="numeral">7</sup>It is an unnecessarily complicated way of writing the update rule, but we will see that this is the correct way to generalize it to higher rank projectors, and later to generalized measurements.</span>:
</p>
\begin{equation}
\rho_i = \frac{\Pi_i \rho \Pi_i}{\tr(\Pi_i \rho)} \label{eq:update}
\end{equation}

<p>
There is another postulate that we can make about quantum theory which is <i>locality</i>. That is, if we have a composite system \(AB\) and we are interested in predicting measurement results for apparatuses acting on \(A\) only, then it should not depend on what happened to \(B\) provided there was no communication between \(A\) and \(B\). This translates into the following. Take \(\rho_{AB}\) a density matrix on \(AB\) and \(\{\Pi_{A,i}\}_i\) a PVM acting on \(A\). Then,
</p>
\begin{equation}
\Pr_{\rho_{AB}}(i) = \tr_A(\Pi_{A,i}(\tr_B(\rho_{AB}))) = \tr_{AB}((\Pi_{A,i}\otimes \one_B)\rho_{AB}),
\end{equation}

<p>
and the post-measurement state for \(A\) is
</p>

\begin{equation}
\rho_{A,i} = \frac{\Pi_i \tr_B(\rho_{AB}) \Pi_i} {\tr_{AB}((\Pi_{A,i}\otimes \one_B)\rho_{AB})}.
\end{equation}
<p>
This can be rewriten as
</p>
\begin{equation}
\rho_{A,i} = \frac{\tr_B((\Pi_i\otimes \one_B) \rho_{AB}(\Pi_i\otimes \one_B))}{\tr_{AB}((\Pi_{A,i}\otimes \one_B)\rho_{AB})}.
\end{equation}
<p>
If we define \(\rho_i = \frac{(\Pi_i\otimes \one_B) \rho_{AB}(\Pi_i\otimes \one_B)}{\tr_{AB}((\Pi_{A,i}\otimes \one_B)\rho_{AB})}\)
we have that \(\rho_{A,i} = \tr_B(\rho_i)\) and that \(\tr_A(\sum_i \rho_i) = \tr_A(\rho) = \rho_B\) so that \(\rho_i\) can be viewed as a candidate post-measurement state for the larger state \(\rho\). This can be shown to be the only possible case by extending the situation where \(\rho_{AB} = \rho_A \otimes \rho_B\) which can be deduced entirely from the locality condition. A similar argument can be made for the post-measurement state of single systems under PVM with higher rank projectors, so that we can now appreciate the fact that Eq. (\ref{eq:update}) applies to these more general situations<label id="fnr.8" for="fnr-in.8.2705295" class="margin-toggle sidenote-number"><sup class="numeral">8</sup></label><input type="checkbox" id="fnr-in.8.2705295" class="margin-toggle" /><span class="sidenote"><sup class="numeral">8</sup>Remember that in fact, \(\Pi_{A,i}\otimes \one_B\) is also a higher-rank projector &#x2013;its rank is \(\rk(\Pi_{A,i})\times\dim(\mathcal H_B)\).</span>.
</p>
</div>
</div>
<div id="outline-container-org92d6aec" class="outline-3">
<h3 id="org92d6aec"><span class="section-number-3">4.2.</span> Positive Operator-Valued Measurements (POVM)</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Consider a system \(A\) in an initial state \(\rho_A\) that can be coupled through a unitary \(U\) to another system \(B\) initially in a fiducial state \(\ketbra 0\) on which a PVM measurement \(\{\Pi_{B,i}\}_i\) can be made. Now we are interested in the understanding the effect on \(A\) of a measurement on \(B\), while the outcome \(i\) is known to the observer looking at \(A\). Then we have, using the same notation as before:
</p>
\begin{equation}
\rho_{i} = \frac{(\one_A \otimes \Pi_{B,i}) U (\rho_A \otimes \ketbra  0_B) U^\dagger (\one_A \otimes \Pi_{B,i})}{\tr_{AB}((\one_A \otimes \Pi_{B,i}) U (\rho_A \otimes \ketbra 0) U^\dagger)}.
\end{equation}
<p>
The density matrix \(\rho_{A,i}\) of \(A\) when \(i\) is observed is thus:
</p>
\begin{equation}
\rho_{A,i} = \tr_B(\rho_i) = \frac{M_i \rho_A M_i^\dagger}{\tr_A (M_i \rho_A M_i^\dagger)},
\end{equation}
<p>
with \(M_i = \bra i_B U \ket 0_B\). Using the circularity of the trace, we obtain that:
</p>
\begin{equation}
\Pr_{\rho_A}(i) = \tr_A (M_i \rho_A M_i^\dagger) =  \tr(E_i\rho),
\end{equation}
<p>
where \(E_i = M_i^\dagger M_i\).
</p>

<p>
Note that each \(E_i\) is positive and that \(\{E_i\}_i\) is a partition of the identity \(\sum_i E_i = \one_A\), hence the term POVM. Further, we see that the operators \(M_i\) are Kraus operators of the evolution starting with \(\rho_A\) and undergoing the measurement process &#x2014; i.e. interaction with \(\ket 0_B\) and measurement along \(\{\Pi_i\}_i\). Conversely, if one is given the operators \(\{E_i\}_i\), there are several possible ways of implementing the measurement with \(M_i' = U M_i\) as \(M_i^{'\dagger} M_i' = M_i^\dagger M_i = E_i\). This means that the post-measurement state cannot be deduced from the POVM alone. Some further details about the implementation are required. Yet, one can use the same purification techniques as before and prove that any POVN can indeed be realized through a unitary evolution and a measurement on the purifying subsystem.
</p>

<div class="definition" id="orgc3e17df">
<p>
Using the notation defined above, the operators \(E_i\) are called <i>POVM elements</i> and the specific \(M_i\) are called instruments.
</p>

</div>
</div>
</div>
</section>

<section id="outline-container-org6ddf272" class="outline-2">
<h2 id="org6ddf272"><span class="section-number-2">5.</span> References</h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li><a href="https://edu.itp.phys.ethz.ch/hs15/QIT/renner_lecture_notes12.pdf">Quantum Information Theory, R. Renner (2013)</a></li>
<li><a href="https://arxiv.org/pdf/1106.1445.pdf">From Classical to Quantum Shannon Theory, M. Wilde (2019)</a></li>
</ul>
</div>
</section>
</article>]]></content><author><name></name></author><category term="Lecture" /><summary type="html"><![CDATA[1. The Church of the larger Hilbert space The axioms of quantum mechanics presented earlier only refer to closed quantum systems. So it is only possible to describe what happens to a system completely isolated from other systems. This seems like a restriction, until you realize that indeed, it is always possible to view an open quantum system as a subsystem of a larger quantum system, which is itself isolated. In case of doubt, you can always throw in the full wave function of the universe! This lecture is about exploiting this ability to better understand states of open quantum systems, their dynamics and the allowed measurements. The term "Church of the larger Hilbert space" refers to this ability to enlarge the Hilbert space by adding more and sometimes fictitious subsystems in order to recover pure states and closed systems dynamics and measurements.]]></summary></entry><entry><title type="html">Closed Quantum Systems</title><link href="http://localhost:4000/2023/09/12/closed_quantum_systems.html" rel="alternate" type="text/html" title="Closed Quantum Systems" /><published>2023-09-12T00:00:00+02:00</published><updated>2023-09-12T00:00:00+02:00</updated><id>http://localhost:4000/2023/09/12/closed_quantum_systems</id><content type="html" xml:base="http://localhost:4000/2023/09/12/closed_quantum_systems.html"><![CDATA[<article id="content" class="content">

Quantum information processing is using quantum mechanics as a way to encode, process and retrieve information using quantum systems. The amount of quantum theory we need to describe these tasks from a computer science perspective is remarkably shallow. It only amounts to the axioms of closed systems quantum mechanics, and in many cases to 2-dimensional systems <label id="fnr.1" for="fnr-in.1.4685317" class="margin-toggle sidenote-number"><sup class="numeral">1</sup></label><input type="checkbox" id="fnr-in.1.4685317" class="margin-toggle" /><span class="sidenote"><sup class="numeral">1</sup>For digging into higher (discrete finite or infinite) dimensional systems, refer to lectures by U. Chabaud and F. Arzani.</span>.

<p>
The aim of this lecture is to:
</p>
<ul class="org-ul">
<li>identify which axiom of quantum mechanics relates to what information processing task;</li>
<li>recall some basic facts about linear algebra that are necessary to develop fluency and intuition.</li>
</ul>

<section id="outline-container-org223160c" class="outline-2">
<h2 id="org223160c"><span class="section-number-2">1.</span> References</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>Preskill's notes <a href="http://www.theory.caltech.edu/~preskill/ph219/chap2_15.pdf">Chapter 2 &#x2013; Foundations I: States and Ensembles, 52 pages (July 2015)</a> and <a href="http://www.theory.caltech.edu/~preskill/ph219/chap3_15.pdf">Chapter 3 &#x2013; Foundations II: Measurement and Evolution, 66 pages (July 2015)</a></li>
<li><a href="https://edu.itp.phys.ethz.ch/hs15/QIT/renner_lecture_notes12.pdf">Quantum Information Theory, R. Renner (2013)</a></li>
<li><a href="https://arxiv.org/pdf/1106.1445.pdf">From Classical to Quantum Shannon Theory, M. Wilde (2019)</a></li>
<li><a href="https://cs.uwaterloo.ca/~watrous/TQI/TQI.pdf">The Theory of Quantum Information, J. Watrous (2018)</a></li>
</ul>
</div>
</section>
<section id="outline-container-org6b80d9e" class="outline-2">
<h2 id="org6b80d9e"><span class="section-number-2">2.</span> Axioms</h2>
<div class="outline-text-2" id="text-2">
<p>
From an operational point of view, a physical theory is a set of mathematical statements that can be combined to predict results of experiments. The crucial ingredient in this definition is the term "physical", as it makes the difference between a mathematical theory and a physical one. The connexion with experiments enforces how it can be disproven. It is unfortunately the only reality check &#x2014; or debugging tool &#x2014; available with what Nature is, and what the theory should describe<label id="fnr.2" for="fnr-in.2.2185609" class="margin-toggle sidenote-number"><sup class="numeral">2</sup></label><input type="checkbox" id="fnr-in.2.2185609" class="margin-toggle" /><span class="sidenote"><sup class="numeral">2</sup>It will be important to remember this as a guideline towards getting to complex tasks using quantum mechanics, and as a strong limitation for how quantum programs can be checked.</span>. In contrast, most mathematical statements have the advantage of being provable from simpler ones. This is a much more desirable situation as we can check the correctness of a statement instead of holding it true until Nature shows that it does not apply for some experiment. 
</p>

<p>
The role of axioms of quantum mechanics is precisely to provide a set of mathematical rules that can be combined <i>ad infinitum</i><label id="fnr.3" for="fnr-in.3.4646706" class="margin-toggle sidenote-number"><sup class="numeral">3</sup></label><input type="checkbox" id="fnr-in.3.4646706" class="margin-toggle" /><span class="sidenote"><sup class="numeral">3</sup>The important point is the claim that the axioms can be combined with one another and still give something meaningful physically. It is an extremely strong statement.</span> to design new experiments for which the predictions will correspond to physical reality &#x2014; unless one of the axioms is wrong. In a sense, it is trying to reduce as much as possible the gap between mathematical and physical theories by allowing to prove instead of just disprove <label id="fnr.4" for="fnr-in.4.7647783" class="margin-toggle sidenote-number"><sup class="numeral">4</sup></label><input type="checkbox" id="fnr-in.4.7647783" class="margin-toggle" /><span class="sidenote"><sup class="numeral">4</sup>Note that this is only partially true, as doing so assumes that axioms are correct. Yet, there is an advantage of doing so as axioms are meant to be easier to check. Reversing this argument, you can see quantum information processing as a way to test these axioms and their composability with one another in an incomparably complex way.</span>.
</p>

<p>
Axioms span 3 physical concepts: 
</p>
<ol class="org-ol">
<li>States</li>
<li>Measurements</li>
<li>Evolutions</li>
</ol>
<p>
The order chosen here is non-standard<label id="fnr.5" for="fnr-in.5.6095030" class="margin-toggle sidenote-number"><sup class="numeral">5</sup></label><input type="checkbox" id="fnr-in.5.6095030" class="margin-toggle" /><span class="sidenote"><sup class="numeral">5</sup>Usually, states are introduced first, followed by evolutions and measurement</span> but intended to emphasize the power and limitations that this axiom introduces for quantum information processing<label id="fnr.6" for="fnr-in.6.558217" class="margin-toggle sidenote-number"><sup class="numeral">6</sup></label><input type="checkbox" id="fnr-in.6.558217" class="margin-toggle" /><span class="sidenote"><sup class="numeral">6</sup>Trying to reduce the number of or simplify axioms is an important task that interests researchers on foundations of quantum mechanics. The reason is that fewer and simpler axioms should be easier to disprove, and provide a better intuition into what can be achieved with quantum mechanics. Examples include: measurement update rule can be derived from measurement + Manzanes paper on replacing the measurement axiom with composability.</span><sup>, </sup><label id="fnr.7" for="fnr-in.7.6902627" class="margin-toggle sidenote-number"><sup class="numeral">7</sup></label><input type="checkbox" id="fnr-in.7.6902627" class="margin-toggle" /><span class="sidenote"><sup class="numeral">7</sup>Alternate theories need to contain classical probability theory.</span><sup>, </sup><label id="fnr.8" for="fnr-in.8.6338781" class="margin-toggle sidenote-number"><sup class="numeral">8</sup></label><input type="checkbox" id="fnr-in.8.6338781" class="margin-toggle" /><span class="sidenote"><sup class="numeral">8</sup>When we imagine disproving a theory we need to pay attention to implicit assumptions. For instance, you could imagine trying to disprove the axioms, by testing them and accumulating statistics. But as such you already use the fact that this is meaningful (i.e. the axioms do not vary in time so that statistics can be accumulated and tell you something about the future). You also imply that axioms can be combined. Mathematically they do, but do they also physically? In a sense there is a 4th axiom that says the other 3 can be combined. One approach to trying to get rid of these implicit asumptions is to use cryptography, considering that only mathematics and locality are trusted, and that Nature is malicious.</span>.
</p>
</div>
</section>

<section id="outline-container-org67c2399" class="outline-2">
<h2 id="org67c2399"><span class="section-number-2">3.</span> Linear algebra detour</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgdb47119" class="outline-3">
<h3 id="orgdb47119"><span class="section-number-3">3.1.</span> Hilbert spaces and Operators</h3>
<div class="outline-text-3" id="text-3-1">
<div text="Inner product space" class="definition" id="orgc7288ba">
<p>
\(V\) is a <i>inner product space</i> over \(\mathbb C\), if it is a vector space over \(\mathbb C\) equipped with an inner product \(\langle \psi, \varphi \rangle\).
</p>

<p>
\(\langle \ , \ \rangle\) is an <i>inner-product</i> if:
</p>
\begin{align}
&amp; \langle \psi, \psi \rangle &gt; 0, \mbox{ for } \psi \neq 0 \\
&amp; \langle \psi, \alpha_1 \varphi_1 + \alpha_2 \varphi_2 \rangle = \alpha_1\langle \psi,  \varphi_1 \rangle + \alpha_2 \langle \psi, \varphi_2 \rangle \\
&amp; \langle \psi, \varphi \rangle = \langle \varphi, \psi \rangle^*
\end{align}

</div>

<div text="Hilbert space" class="definition" id="org86cf7a1">
<p>
\(V\) is a <i>complex Hilbert space</i>, if:
</p>
<ul class="org-ul">
<li>\(V\) is a inner-product space over \(\mathbb C\);</li>
<li>It is complete for the norm \(\|\psi \| = \sqrt{\langle \psi, \psi \rangle}\) (i.e. Cauchy sequences for \(\| . \|\) converge in \(V\)). This is always the case for finite dimensional \(V\).</li>
</ul>

</div>

<div text="Linear operators" class="definition" id="orgcdf35e6">
<p>
With \(\mathcal H\) and \(\mathcal H'\) being Hilbert spaces,
</p>
<ul class="org-ul">
<li>Linear operators from \(\mathcal H\) to \(\mathcal H'\) are <i>homomorphisms</i> \(Hom(\mathcal H, \mathcal H')\);</li>
<li>Linear operators where \(\mathcal H = \mathcal H'\) are <i>endomorphisms</i> \(End(\mathcal H)\);</li>
<li>The <i>adjoint</i> of \(O \in Hom(\mathcal H, \mathcal H')\) is the unique operator \(O^\dagger\) in \(Hom(\mathcal H', \mathcal H)\) such that \(\langle\psi', O \psi \rangle = \langle O^\dagger \psi', \psi\rangle\) for \(\psi \in \mathcal H, \ \psi' \in \mathcal H'\). When \(O\) is represented as a matrix, that of \(O^\dagger\) is the conjugate transpose;</li>
<li>For \(O \in End(\mathcal H)\), \(O\) is <i>normal</i> if \(OO^\dagger = O^\dagger O\);</li>
<li>For \(O \in End(\mathcal H)\), \(O\) is <i>unitary</i> if \(OO^\dagger = O^\dagger O = \one\);</li>
<li>For \(O \in End(\mathcal H)\), \(O\) is <i>Hermitian</i> if \(O = O^\dagger\);</li>
<li>For \(O \in End(\mathcal H)\), \(O\) is positive if \(\forall \psi, \ \langle \psi , O \psi\rangle \geq 0\),<label id="fnr.9" for="fnr-in.9.6063940" class="margin-toggle sidenote-number"><sup class="numeral">9</sup></label><input type="checkbox" id="fnr-in.9.6063940" class="margin-toggle" /><span class="sidenote"><sup class="numeral">9</sup>Positive operators are Hermitian. It's usually denoted \(O \geq 0\), and for \(O\) and \(O'\), \(O \geq O' \Leftrightarrow O-O' \geq 0\).</span> and <i>positive semi-definite</i> when \(\langle \psi , O \psi\rangle &gt; 0, \ \psi \neq 0\);</li>
<li>For \(O \in End(\mathcal H)\), \(O\) is a projector<label id="fnr.10" for="fnr-in.10.3764816" class="margin-toggle sidenote-number"><sup class="numeral">10</sup></label><input type="checkbox" id="fnr-in.10.3764816" class="margin-toggle" /><span class="sidenote"><sup class="numeral">10</sup>Projectors are positive operators.</span> if \(O^2 = O\).</li>
</ul>

</div>

<div text="Bases" class="definition" id="orgdabc7c4">
<p>
A basis \(\{\psi_i\}_i\) of \(\mathcal H\) is <i>orthonormal</i> if \(\langle \psi_i, \psi_j \rangle = \delta_{i,j}\).
</p>

</div>

<div text="Matrix representation" class="remark" id="org2814ad4">
<p>
Given an orthonormal basis \(\{\psi_i\}_i\), the matrix representation of \(O \in End(\mathcal H)\) is \(O_{i,j} = \langle \psi_i, O \psi_j\rangle\). \(O\) is <i>diagonal wrt \(\{\psi_i\}_i\)</i> if the matrix \(O_{i,j}\) is diagonal<label id="fnr.11" for="fnr-in.11.1493979" class="margin-toggle sidenote-number"><sup class="numeral">11</sup></label><input type="checkbox" id="fnr-in.11.1493979" class="margin-toggle" /><span class="sidenote"><sup class="numeral">11</sup>Always keep in mind that \(O_{i,j}\) is <i>not</i> \(O\), e.g. \(O_{i,j}\) depends on the chosen basis!</span>. 
</p>

</div>

<div text="Trace" class="definition" id="orgdb60279">
<p>
The trace of \(A \in End(\mathcal H)\) is a linear functional on endomorphisms such that \(f(xy) = f(yx)\) and such that \(f(\one) = n\) for \(\mathcal H\) an \(n\)-dimensional Hilbert space.
</p>

</div>
<p>
Note that \(\tr\) is invariant under conjugation by a unitary. Note also that \(\tr (A\otimes B) = \tr(A)\tr(B)\).
</p>

<div text="Froebenius inner product" class="definition" id="org35912a3">
<p>
Given \(A\) and \(B\) complex \(n\times m\) complex matrices form a vector space \(V\). The <i>Froebenius inner product</i> of \(A\) and \(B\) in \(V\) is defined by:
</p>
\begin{equation}
\langle A,B\rangle_F = \tr(A^\dagger B).
\end{equation}

</div>
<p>
Note that it corresponds to the regular inner product if we write the matrices as vectors using the basis \(e_{i,j}\) corresponding to a matrix with a \(1\) at position \((i,j)\) and \(0\) elsewhere.
</p>

<p>
It naturally defines a norm over matrices, the Froebenius norm:
</p>
<div text="Froebenius norm" class="definition" id="orge96b93a">
\begin{equation}
 \|A\|_F = \sqrt{\langle A, A \rangle_F}.
\end{equation}

</div>
<p>
\(\| A \|_F\) is also denoted \(\| A \|_2\) as it is the 2-norm of the spectral decomposition<label id="fnr.12" for="fnr-in.12.5175853" class="margin-toggle sidenote-number"><sup class="numeral">12</sup></label><input type="checkbox" id="fnr-in.12.5175853" class="margin-toggle" /><span class="sidenote"><sup class="numeral">12</sup>See below</span> of \(A\) when the operator is normal.
</p>


<p>
Note that these definitions can be extended to the infinite-dimensional case and constitute the <i>Hilbert-Schmidt</i> inner product and norm. One important property of Froebenius / Hilbert-Schmidt inner product is that if \(A,B \geq 0\) then \(\langle A, B \rangle = \tr (A B) \geq 0\).
</p>

<p>
Hermitian matrices form a subspace of all possible matrices. With the Hilbert-Schmidt inner product, the Hermitian matrices form a real Hilbert space. A convenient basis of such space for the \(2^n\) dimensional case are tensor products of the Pauli matrices<label id="fnr.13" for="fnr-in.13.8355323" class="margin-toggle sidenote-number"><sup class="numeral">13</sup></label><input type="checkbox" id="fnr-in.13.8355323" class="margin-toggle" /><span class="sidenote"><sup class="numeral">13</sup>Generalization to arbitrary dimensions are in section 4.1.7 of [Ren]</span>:
</p>
\begin{align}
I &amp; =  \begin{bmatrix} 1 &amp;  0 \\ 0 &amp; 1 \end{bmatrix} \\
X &amp; =  \begin{bmatrix} 0 &amp;  1 \\ 1 &amp; 0 \end{bmatrix} \\
Y &amp; = i\begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix} \\
Z &amp; =  \begin{bmatrix} 1 &amp;  0 \\ 0 &amp; -1 \end{bmatrix}
\end{align}
<p>
Note that these are not normalized. For the Hilbert-Schmidt norm, they need to be multiplied by \(\frac{1}{\sqrt 2}\). Additionally, all of them have vanishing trace, except the identity. 
</p>

<div text="Trace norm" class="definition" id="orgcb74e59">
<p>
The <i>trace norm</i> of an operator \(A\) is
</p>
\begin{equation}
\| A \|_1 = \tr |A|,
\end{equation}
<p>
where  \(|A| = \sqrt{A^\dagger A}\).
</p>

</div>
<p>
For \(A\) normal, \(\|A\|_1\) is the trace norm of the spectral decomposition<label id="fnr.14" for="fnr-in.14.2733231" class="margin-toggle sidenote-number"><sup class="numeral">14</sup></label><input type="checkbox" id="fnr-in.14.2733231" class="margin-toggle" /><span class="sidenote"><sup class="numeral">14</sup>See below</span> of \(A\).
</p>

<div class="property" id="orgbe5d1aa">
<p>
The trace norm satisfies:
</p>
\begin{equation}
 \| A \|_1 = \max_U |\tr(UA)| 
\end{equation}
<p>
where \(U\) are unitaries.
</p>

</div>
</div>
</div>

<div id="outline-container-org4383c72" class="outline-3">
<h3 id="org4383c72"><span class="section-number-3">3.2.</span> Braket notation</h3>
<div class="outline-text-3" id="text-3-2">
<div text="Braket notation" class="definition" id="orgbb6ad30">
<p>
A vector \(\vec \psi \in \mathcal H\) can be seen as an homomorphism \(\ket \psi \in Hom(\mathbb C, \mathcal H)\):
</p>
\begin{equation}
\alpha \in \mathbb C \xrightarrow{\ket \psi} \alpha \vec \psi \in \mathcal H
\end{equation}
<p>
The adjoint \(\ket \psi^\dagger\) is denoted \(\bra \psi\) and is defined as<label id="fnr.15" for="fnr-in.15.7896120" class="margin-toggle sidenote-number"><sup class="numeral">15</sup></label><input type="checkbox" id="fnr-in.15.7896120" class="margin-toggle" /><span class="sidenote"><sup class="numeral">15</sup>\(\ket\psi: \mathbb C \rightarrow \mathcal H\), \(\ket\psi^\dagger: \mathcal H \rightarrow \mathbb C\), \(\langle \vec\psi \alpha, \vec \varphi\rangle = \alpha^* \langle \vec \psi, \vec \varphi \rangle  = \alpha^*.\braket{\psi}{\varphi}\). Simplifying by \(\alpha^*\) gives the result.</span>:
\[\bra \psi: \ \vec \varphi \rightarrow  \langle \vec \psi, \vec \varphi\rangle.\]
</p>

</div>
</div>
</div>
</section>

<section id="outline-container-org3880d8c" class="outline-2">
<h2 id="org3880d8c"><span class="section-number-2">4.</span> States of quantum systems</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org6895ff9" class="outline-3">
<h3 id="org6895ff9"><span class="section-number-3">4.1.</span> Pure states</h3>
<div class="outline-text-3" id="text-4-1">
<p>
The state of a system is its complete description. Concretely, if you can write the state of a system on a piece of paper, you can predict, through mathematical calculations, the result of any physical experiments that you could perform on the system. Here, the notion of complete description means that there is no uncertainty or no ignorance in the system that could be reduced. In such case, the state \(\ket \psi\) of a system is a ray in a Hilbert space \(\mathcal H\). That is:
</p>
\begin{align}
&amp; \|\ket \psi \| = \sqrt{\braket{\psi}{\psi}} = 1 \\
&amp; \ket \varphi \in \overline{\ket \psi} \Leftrightarrow \, \exists \alpha \neq 0, \ \ket \varphi = \alpha \ket \psi.
\end{align}
<p>
Note that usually we simply identify the equivalence class \(\overline{\ket \psi}\) with one of its representative normalized vector, here \(\ket \psi\). The reason why global phases can be ignored and why there is a strong motivation for taking the norm of the vector equal to one will become apparent when considering the prediction of experimental results.
</p>
</div>
</div>

<div id="outline-container-org42ffd88" class="outline-3">
<h3 id="org42ffd88"><span class="section-number-3">4.2.</span> Mixed states</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Although the presentation above is standard in many textbooks, in practice, it is often replaced by a subtly different one. The state of a system is then defined as the mathematical representation of what knowledge an observer has about it. In this definition, states become relative to each observer<label id="fnr.16" for="fnr-in.16.9454099" class="margin-toggle sidenote-number"><sup class="numeral">16</sup></label><input type="checkbox" id="fnr-in.16.9454099" class="margin-toggle" /><span class="sidenote"><sup class="numeral">16</sup>One might question here what kind of status the observer has from within quantum mechanics. Very rapidly, this should lead to questionning how systems are defined. You might end up being forced to consider the whole universe as the only physical system that makes sense. But then, what do you do with special relativity? What do you mean by the state of the universe when it is not accessible to you? These foundational questions will not be addressed here. Quantum mechanics will be taken from a purely operational view. Yet, it does not mean that Quantum Information Processing cannot be used to address them. One of the most celebrated example is the result \(MIP^* = RE\) from complexity theory that has implications into the structure of Hilbert spaces for infinite-dimensional composite systems.</span><sup>, </sup><label id="fnr.17" for="fnr-in.17.2442226" class="margin-toggle sidenote-number"><sup class="numeral">17</sup></label><input type="checkbox" id="fnr-in.17.2442226" class="margin-toggle" /><span class="sidenote"><sup class="numeral">17</sup>The additional reason for preferring a definition where states are explicitely observer-dependent is that it emphasizes the nature of states: they are the consequence of the observer's relation to a system, rather than representing a pre-existing property of the system itself without reference to an observer. This epistemic vs ontic view of quantum mechanics has been a heated debate since the early days of quantum mechanics. See for instance <a href="https://doi.org/10.1016/j.shpsb.2006.10.007">https://doi.org/10.1016/j.shpsb.2006.10.007</a>.</span>. Yet, they retain their operational property of allowing the observer to predict the results of any experiment it could perfom. The interest of such definition is that it naturally incorporates the lack of knowledge an observer might have about the system. Its predictions will be worse than for a well-informed one. This interpretation of quantum states will be convenient to represent the views two parties with different knowledge about the same system. In such case, a state \(\rho\) is best represented by a density matrix acting on a Hilbert space \(\mathcal H\) &#x2014; the same as the one used before. That is:
</p>
\begin{align}
\rho &amp; = \rho^\dagger \\
\rho &amp; \geq 0 \\
\tr \rho &amp; = 1.
\end{align}

<p>
The obvious question is how does this different representation of states connect to the state vector. One way of making this link is by explicitely working out the predictions on measurements they give and recognizing when they should be identical (see below).
</p>

<p>
An important class of mixed states is:
</p>
<div text="Classical states" class="definition" id="org1538ce3">
<p>
For a chosen orthonormal basis \(\{\ket i\}\), <i>classical states</i> are defined as linear combinations of the corresponding projectors \(\{\ketbra i\}\):
\[\sum_i p_i \ketbra i, \]
where \(p_i\) is a probability distribution. 
</p>

</div>
<p>
The reason why these states are called classical will be justified by what can be predicted about them.<label id="fnr.18" for="fnr-in.18.1005898" class="margin-toggle sidenote-number"><sup class="numeral">18</sup></label><input type="checkbox" id="fnr-in.18.1005898" class="margin-toggle" /><span class="sidenote"><sup class="numeral">18</sup>Spoiler: they are perfectly distinguishable as it would be expected for classical random variables. As a consequence, it is easy to embed a discrete classical probability distribution within a quantum state.</span>
</p>
</div>
</div>

<div id="outline-container-org0cbef3c" class="outline-3">
<h3 id="org0cbef3c"><span class="section-number-3">4.3.</span> Composite systems</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Lastly, we will often need to consider composite systems, i.e. systems with subparts. Here, quantum mechanics says that the state of a composite quantum system \(\mathcal A - \mathcal B\) is a ray (resp. density matrix) in the tensor product of the Hilbert spaces of each subsystem, i.e. \(\mathcal H_{\mathcal A} \otimes \mathcal H_{\mathcal B}\).
</p>

<p>
This axiom plays a crucial role in quantum information processing:
</p>
<ul class="org-ul">
<li>It is an essential ingredient for complexity theory as it defines elementary resources that will be counted to assess the complexity / efficiency of a given algorithm or protocol for resolving a task. In particular, it states that although \(n\) 2-dimensional subsystems span a \(2^n\)p-dimensional Hilbert space, they are only corresponding to a linear number of resources used<label id="fnr.19" for="fnr-in.19.3092161" class="margin-toggle sidenote-number"><sup class="numeral">19</sup></label><input type="checkbox" id="fnr-in.19.3092161" class="margin-toggle" /><span class="sidenote"><sup class="numeral">19</sup>This is in complete analogy with classical complexity theory, where bits a resources allowing access to exponentially many bit-string values.</span>.</li>
<li>It creates a very rich structure if one associates to each subsystem a notion of locality. In such case, one can wonder what are the set of bipartite states that can be accessed through local operations and classical operations, and what kind of states require non-local operations to be created from independent subsystems.</li>
</ul>
</div>
</div>
</section>

<section id="outline-container-org3f9ff95" class="outline-2">
<h2 id="org3f9ff95"><span class="section-number-2">5.</span> Predicting results from experiments</h2>
<div class="outline-text-2" id="text-5">
<p>
An experiment is a physical setup that interacts with a quantum system and has several outcomes. The goal of the measurement axiom is to (1) specify mathematically how these physical setups are represented, (2) what can be predicted about these outcomes, and (3) how to do so.
</p>

<ol class="org-ol">
<li>A (projector-valued) measurement (PVM) \(\mathcal M\) is a set of orthogonal projectors summing to \(\one\): \[\mathcal M = \{M_i\}_i, \ M_i M_j = \delta_{i,j} M_i, \ \sum_i M_i = \one.\] The projector \(M_i\) is said to correspond to <i>outcome</i> \(i\).</li>
<li>Given the state of a quantum system defined on the same Hilbert space as \(\mathcal M\), it is possible to compute the probability of getting outcome \(i\).</li>
<li>if the state is in state \(\ket \psi\)<label id="fnr.20" for="fnr-in.20.9843329" class="margin-toggle sidenote-number"><sup class="numeral">20</sup></label><input type="checkbox" id="fnr-in.20.9843329" class="margin-toggle" /><span class="sidenote"><sup class="numeral">20</sup>Remember the state needs to be normalized.</span> (resp. \(\rho\)) the probability of getting \(i\) is:</li>
</ol>
\begin{equation}
\Pr(i|\ket \psi) = \bra \psi M_i \ket \psi, \mbox{ resp. } \Pr(i|\rho) = \tr(\rho M_i).
\end{equation}

<p>
The presentation adopted here focuses on PVM instead of observables &#x2014; i.e. hermitian matrices. The reason is that several observables lead to the same measurement by just changing the eigenvalues of the operator. The observables \(\sum_i \lambda_i M_i\) and \(\sum_i \lambda'_i M_i\) contain the same amount of information as long as the \(\lambda\) coefficients do not create spurious degeneracies. As for states, we can think of PVM as normalized representatives for all the possible observables acting on \(\mathcal H\). 
</p>

<p>
Additionally, it emphasizes that there is no such thing as the measurement of the expectation value of an observable. There are only detector clicks that give one of the discrete outcomes \(i\). The measurement of an expectation value is the act of repeating such measurement many times for the same reprepared state and accumulating the statistics for each of the outcomes. In other words, expectation values are averages, while quantum mechanics &#x2014; through this postulate &#x2014; gives us the ability to sample from a probability distribution. This is of course much more information than just the average value of the probability distribution. Such distinction will become crucial when counting resources for performing discrimination or estimation tasks, and of course when evaluating the complexity of an algorithm.
</p>

<p>
It is customary to introduce thet post-measurement state at this stage. It is in fact not necessary &#x2014; see for instance <a href="https://cs.uwaterloo.ca/~watrous/TQI/TQI.pdf">The Theory of Quantum Information, J. Watrous (2018)</a>. Yet, it is useful to understand what is indeed meant by post-measurement state. Indeed, in the lab, physicists have observed that when measurements are very carefully implemented, they can be non destructive in the sense that once a measurement has been done, not only does the quantum system survive to it, but an immediate subsequent measurement gives the same result. Applying this to PVMs shows that when they are implemented in such non destructive way, the state of the system just after the measurement outcome \(i\) has been observed must be \(M_i \ket \psi / \| M_i \ket \psi\|\) &#x2014; where \(M_i\) is the corresponding projector and \(\ket \psi\) is the state of the system. More generally, for a density matrix \(\rho\), the post-measurement state for a non-destructive PVM \(\{M_i\}_i\) where \(i\) is observed is given by
</p>
\begin{equation}
\frac{M_i \rho M_i }{\frac \tr (M_i \rho)}.
\end{equation}
<p>
Nonetheless, one needs to remember that this is <i>only</i> if the measurement is implemented in a non-destructive way. It is a lot of efforts to do it, and most often than not, the quantum system either does not survive or is experiencing additional transformations that yield to further evolutions. 
</p>
</div>
</section>

<section id="outline-container-orgbadf71b" class="outline-2">
<h2 id="orgbadf71b"><span class="section-number-2">6.</span> Evolving states</h2>
<div class="outline-text-2" id="text-6">
<p>
The axiom governing the evolution of quantum systems can be summarized as:
</p>
<ul class="org-ul">
<li>allowed evolutions for a system with Hilbert space \(\mathcal H\) are linear: \(\mathcal U[\ket \psi + \ket \varphi] = \mathcal U[\ket\psi] + U[\ket \varphi]\)</li>
<li>allowed evolutions should map states to states, i.e. a normalized representative of a ray should map to another normalized representative of a ray:</li>
</ul>
\begin{align}
&amp; \forall \ket \psi, \ket \varphi, \ \exists \mathcal U, \ \mathcal U[\ket \psi] = \ket \varphi \\
&amp; \forall \mbox{ allowed } \mathcal U, \forall \ket \psi, \ \langle \mathcal U[\ket\psi], \mathcal U[\ket\psi]\rangle = 1
\end{align}
<p>
This means that the allowed evolutions are the whole unitary group acting on \(\mathcal H\).
</p>

<p>
In most aspects of quantum information processing from a computer science perspective, it will suffice to consider such discrete evolutions. Yet, implementing these evolutions using real physical systems requires to describe continuous time control and evolutions. Schroedinger's equation can be recovered by considering a parametrization of the unitary group in terms of continuous paths using infinitesimal transformation generators. This would lead to construct the associated Lie algebra, and the exponential function that appears in the solution to Schroedinger's equation. 
</p>
</div>
</section>

<section id="outline-container-org0537eb8" class="outline-2">
<h2 id="org0537eb8"><span class="section-number-2">7.</span> More on state vectors and density matrices</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org2e115d1" class="outline-3">
<h3 id="org2e115d1"><span class="section-number-3">7.1.</span> Decomposing density matrices</h3>
<div class="outline-text-3" id="text-7-1">
<p>
Because density matrices in the \(2^n\)-dimensional setting are positive semi-definite and have trace 1, they can be conveniently expressed using the Pauli matrices. For a 2-dimensional system (qubit) we have:
</p>
\begin{equation}
\rho = \frac{1}{2}(I + x X + y Y + z Z)
\end{equation}
<p>
Note that here, the normalization factor<label id="fnr.21" for="fnr-in.21.1693503" class="margin-toggle sidenote-number"><sup class="numeral">21</sup></label><input type="checkbox" id="fnr-in.21.1693503" class="margin-toggle" /><span class="sidenote"><sup class="numeral">21</sup>Always pay attention to the definition of Pauli matrices when reading a research paper, as it might not be consistent throughout the whole paper! 3 normalization conventions co-exist: \(\tr I = 1\), \(\tr I = 2\) and \(\tr I = 2/\sqrt 2\).</span> of the Pauli matrices is implicitely defined to be 1, i.e. \(\tr \rho = 1\). The positivity criterion imposes that \(x^2+y^2+z^2\leq 1\). This is the usual Bloch sphere representation of qubits.
</p>

<p>
Note that using the generalization of the Pauli matrices to decompose Hermitian matrices, this representation can be generalized to arbitrary dimensions.
</p>
</div>
</div>

<div id="outline-container-org144986c" class="outline-3">
<h3 id="org144986c"><span class="section-number-3">7.2.</span> State vectors and density matrices</h3>
<div class="outline-text-3" id="text-7-2">
<p>
The correspondence between state vectors and density matrices can be established from an operational point of view. Given a state vector, we seek what density matrix it corresponds to by requiring that they give the same predictions for the same PVMs.
</p>

<p>
Let \(\ket \psi\) be a state vector. Consider the binary PVM \(\mathcal M = \{M_0, M_1\} \coloneqq  \{\Pi_{\ket \psi}, \one - \Pi_{\ket \psi}\}\). Then, the measurement postulate gives that the distribution of outcomes will be:
</p>
\begin{align}
\Pr_{\ket \psi}(0) &amp; = 1 \\
\Pr_{\ket \psi}(1) &amp; = 0.
\end{align}
<p>
Now, taking the density matrix approach, we should have
</p>
\begin{align}
\tr(\rho M_0) &amp; = 1 \\
\tr(\rho M_1) &amp; = 0.
\end{align}
<p>
Using the fact that \(\rho = \rho^\dagger\) we recognize that \(\tr(\rho M_0) = \langle \rho, M_0 \rangle_F\), which imposes together with the trace 1 condition that \(\rho = M_0 = \ketbra \psi\).
</p>

<p>
Hence, state vectors correspond to density matrices that are rank-one projectors. The converse is also true. Using the fact that \(\Pi^2 = \Pi\) for projectors and the fact that \(\mathrm{rk} (\Pi) = \tr (\Pi)\) we obtain the <i>purity</i> criterion:
\[\tr \rho^2 = 1 \Leftrightarrow \ \exists \ket \psi, \rho = \ketbra \psi.\]
</p>

<p>
This identification being done, we can derive the evolution postulate for density matrices from the one on states:
</p>
\begin{equation}
\rho \rightarrow U\rho U^\dagger.
\end{equation}
</div>
</div>

<div id="outline-container-orge3cd094" class="outline-3">
<h3 id="orge3cd094"><span class="section-number-3">7.3.</span> Density matrices as ensemble preparations</h3>
<div class="outline-text-3" id="text-7-3">
<p>
To further understand what are density matrices, we can consider a scenario where a source can prepare one of many pure states \(\ket{\psi_i}\)  with probability \(p_i\). If one state prepared by the source is given to you (without you knowing the index \(i\)), what is your description of the state of the received quantum system?
</p>

<p>
One way to approach the question is to ask yourself what can we predict about the system. Take \(\mathcal M \coloneqq \{M_j\}_j = \{\ketbra {\varphi_j}\}_j\) a PVM, then:
</p>
\begin{equation}
\Pr(\mbox{outcome }j) = \sum_i p_i \braket{\varphi_j}{\psi_i}.
\end{equation}
<p>
But as we have seen this is the same as \(\sum_i p_i \tr(M_j \ketbra{\psi_i}{\psi_i})\). Using the linearity of the trace we have:
</p>
\begin{align}
\Pr(\mbox{outcome }j) &amp; = \tr (M_j \rho) \\
\rho &amp; = \sum_i p_i \ketbra{\psi_i}{\psi_i}.
\end{align}
<p>
Because this holds for \(\mathcal M\) an arbitrary PVM, it is possible to choose several of them so that this \(\rho\) is unique.
</p>

<p>
Note however, that while \(\rho\) is unique, each \(\rho\) can correspond to many ensemble preparations. 
</p>
</div>
</div>
</section>

<section id="outline-container-org7e8cdfc" class="outline-2">
<h2 id="org7e8cdfc"><span class="section-number-2">8.</span> More on operators</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org66ec768" class="outline-3">
<h3 id="org66ec768"><span class="section-number-3">8.1.</span> Schur decomposition</h3>
<div class="outline-text-3" id="text-8-1">
<p>
Let \(A\in End(\mathcal H)\), then
</p>
\begin{equation}
A = U T U^\dagger,
\end{equation}
<p>
where \(U\) is unitary and \(T\) is upper triangular.
</p>

<p>
This decomposition is obtained by picking up an eigenvalue of \(A\) and considering the direct sum of the eigenspace associated to this eigenvalue and its orthogonal complement. Then, the decomposition is repeated on the complement.
</p>
</div>
</div>

<div id="outline-container-orga8e2dbd" class="outline-3">
<h3 id="orga8e2dbd"><span class="section-number-3">8.2.</span> Spectral decomposition</h3>
<div class="outline-text-3" id="text-8-2">
<p>
Let \(A\) be a normal operator in \(End(\mathcal H)\). Then
</p>
\begin{equation}
 A = U D U^\dagger, 
\end{equation}
<p>
for \(U\) a unitary and \(D\) a diagonal matrix.
</p>

<p>
The proof follows from Schur's decomposition, where normality implies that \(T\) is also normal. Using that \(T\) is upper triangular, a direct inspection shows that normality imposes that \(T\) is diagonal.
</p>
</div>
</div>

<div id="outline-container-orgd1a759f" class="outline-3">
<h3 id="orgd1a759f"><span class="section-number-3">8.3.</span> Singular value decomposition</h3>
<div class="outline-text-3" id="text-8-3">
<p>
Let \(A \in Hom(\mathcal H, \mathcal H')\), then
</p>
\begin{equation}
A = V D U,
\end{equation}
<p>
where \(U\) and \(V\) are unitaries.
</p>

<p>
The proof is obtained by applying the spectral theorem to \(A^\dagger A\).
</p>
</div>
</div>

<div id="outline-container-orgafe4009" class="outline-3">
<h3 id="orgafe4009"><span class="section-number-3">8.4.</span> Polar decomposition</h3>
<div class="outline-text-3" id="text-8-4">
<p>
Let \(A \in End(\mathcal H)\), then
</p>
\begin{equation}
 A =  \sqrt{AA^\dagger} U = U \sqrt{A^\dagger A}, 
\end{equation}
<p>
For \(U\) a unitary. Note that \(\sqrt{AA^\dagger}\) is positive semi-definite.
</p>

<p>
The proof is obtained by using the SVD for \(A\), and inserting either \(V^{'\dagger} V'\) or \(U' U^{'\dagger}\) from the SVD to obtain \(V'DV^{'\dagger}\) or \(U^{'\dagger} D U\) terms that correspond to \(\sqrt{AA^\dagger}\) and \(\sqrt{A^\dagger A}\).
</p>
</div>
</div>
</section>

<section id="outline-container-orge856e80" class="outline-2">
<h2 id="orge856e80"><span class="section-number-2">9.</span> More on composite systems</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-org5ac39f9" class="outline-3">
<h3 id="org5ac39f9"><span class="section-number-3">9.1.</span> Schmidt decomposition</h3>
<div class="outline-text-3" id="text-9-1">
<p>
When we have a pure state for a composite system on \(\mathcal H \otimes \mathcal H'\), we might want to pickup a separable basis<label id="fnr.22" for="fnr-in.22.9322620" class="margin-toggle sidenote-number"><sup class="numeral">22</sup></label><input type="checkbox" id="fnr-in.22.9322620" class="margin-toggle" /><span class="sidenote"><sup class="numeral">22</sup>A basis of the form \(\ket {\psi_i} \otimes \ket{\psi'_j}\) to express the state.</span>. Are all these bases equivalent or is one better than the others? In fact there is a family of bases that stands out and which is defined using Schmidt decomposition
</p>

<div text="Schmidt decomposition" class="theorem" id="org92f07d2">
<p>
For \(\ket\Psi \in \mathcal H \otimes \mathcal H'\), there exists \(\{\ket {\psi_i}\}_i\) and \(\{\ket {\psi'_i}\}_i\) orthonormal sets of \(\mathcal H\) and \(\mathcal H'\) respectively such that
</p>
\begin{equation}
 \ket \Psi = \sum_i \lambda_i \ket{\psi_i} \otimes \ket{\psi'_i}. 
\end{equation}

</div>
<p>
Note that there is a <i>single</i> summation index.
</p>

<div class="proof" id="org877ef8b">
<p>
Pick two bases of \(\mathcal H\) and \(\mathcal H'\). Then
</p>
\begin{equation}
 \ket \Psi = \sum_{i,j} \gamma_{i,j} \ket{\varphi_i} \otimes \ket{\varphi'_j}.
\end{equation}
<p>
By identifying \(\ket{\varphi_i} \otimes \ket{\varphi'_j}\) and \(\ketbra{\varphi_i}{\varphi'_j}\), \(\ket \Psi\) can be identified with a matrix \(P\)<label id="fnr.23" for="fnr-in.23.1048760" class="margin-toggle sidenote-number"><sup class="numeral">23</sup></label><input type="checkbox" id="fnr-in.23.1048760" class="margin-toggle" /><span class="sidenote"><sup class="numeral">23</sup>This same trick will be used to derive the Choi-Jamiolkowski isomorphism representation of Completely Positive Trace Preserving maps.</span>. This matrix can be decomposed using the SVD, that is:
</p>
\begin{equation}
P = U \Pi V^\dagger,
\end{equation}
<p>
where \(\Pi\) is positive semi-definite. Taking the column vectors \(\ket{\psi_i}\) and \(\ket{\psi'_i}\) of \(U\) and \(V\) corresponding to the non-zero entries \(\lambda_i\) of \(\Pi\) we can now write
</p>
\begin{equation}
 P = \sum_i \lambda_i \ketbra{\psi_i}{\psi'_i},
\end{equation}
<p>
which translates back to
</p>
\begin{equation}
 \ket \Psi = \sum_i \lambda_i \ket{\psi_i} \otimes \ket{\psi'_i}. 
\end{equation}

</div>

<p>
As a result, the number of components of the state vector for a composite system written in a separable basis can always be brought back to \(\min (\dim \mathcal H, \dim\mathcal H')\).
</p>
</div>
</div>
<div id="outline-container-orga835bb6" class="outline-3">
<h3 id="orga835bb6"><span class="section-number-3">9.2.</span> Entanglement</h3>
<div class="outline-text-3" id="text-9-2">
<p>
For pure states, an entangled state corresponds to a state with <i>Schmidt number</i> &#x2014; the number of non-zero coefficient in its Schmidt decomposition &#x2014; strictly greater than 1.
</p>
</div>
</div>
</section>
<!-- Footnotes --><!-- 
<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">For digging into higher (discrete finite or infinite) dimensional systems, refer to lectures by U. Chabaud and F. Arzani.</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">It will be important to remember this as a guideline towards getting to complex tasks using quantum mechanics, and as a strong limitation for how quantum programs can be checked.</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">The important point is the claim that the axioms can be combined with one another and still give something meaningful physically. It is an extremely strong statement.</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Note that this is only partially true, as doing so assumes that axioms are correct. Yet, there is an advantage of doing so as axioms are meant to be easier to check. Reversing this argument, you can see quantum information processing as a way to test these axioms and their composability with one another in an incomparably complex way.</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Usually, states are introduced first, followed by evolutions and measurement</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Trying to reduce the number of or simplify axioms is an important task that interests researchers on foundations of quantum mechanics. The reason is that fewer and simpler axioms should be easier to disprove, and provide a better intuition into what can be achieved with quantum mechanics. Examples include: measurement update rule can be derived from measurement + Manzanes paper on replacing the measurement axiom with composability.</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7" role="doc-backlink">7</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Alternate theories need to contain classical probability theory.</p></div></div>

<div class="footdef"><sup><a id="fn.8" class="footnum" href="#fnr.8" role="doc-backlink">8</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">When we imagine disproving a theory we need to pay attention to implicit assumptions. For instance, you could imagine trying to disprove the axioms, by testing them and accumulating statistics. But as such you already use the fact that this is meaningful (i.e. the axioms do not vary in time so that statistics can be accumulated and tell you something about the future). You also imply that axioms can be combined. Mathematically they do, but do they also physically? In a sense there is a 4th axiom that says the other 3 can be combined. One approach to trying to get rid of these implicit asumptions is to use cryptography, considering that only mathematics and locality are trusted, and that Nature is malicious.</p></div></div>

<div class="footdef"><sup><a id="fn.9" class="footnum" href="#fnr.9" role="doc-backlink">9</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Positive operators are Hermitian. It's usually denoted \(O \geq 0\), and for \(O\) and \(O'\), \(O \geq O' \Leftrightarrow O-O' \geq 0\).</p></div></div>

<div class="footdef"><sup><a id="fn.10" class="footnum" href="#fnr.10" role="doc-backlink">10</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Projectors are positive operators.</p></div></div>

<div class="footdef"><sup><a id="fn.11" class="footnum" href="#fnr.11" role="doc-backlink">11</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Always keep in mind that \(O_{i,j}\) is <i>not</i> \(O\), e.g. \(O_{i,j}\) depends on the chosen basis!</p></div></div>

<div class="footdef"><sup><a id="fn.12" class="footnum" href="#fnr.12" role="doc-backlink">12</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">See below</p></div></div>

<div class="footdef"><sup><a id="fn.13" class="footnum" href="#fnr.13" role="doc-backlink">13</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Generalization to arbitrary dimensions are in section 4.1.7 of [Ren]</p></div></div>

<div class="footdef"><sup><a id="fn.14" class="footnum" href="#fnr.14" role="doc-backlink">14</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">See below</p></div></div>

<div class="footdef"><sup><a id="fn.15" class="footnum" href="#fnr.15" role="doc-backlink">15</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">\(\ket\psi: \mathbb C \rightarrow \mathcal H\), \(\ket\psi^\dagger: \mathcal H \rightarrow \mathbb C\), \(\langle \vec\psi \alpha, \vec \varphi\rangle = \alpha^* \langle \vec \psi, \vec \varphi \rangle  = \alpha^*.\braket{\psi}{\varphi}\). Simplifying by \(\alpha^*\) gives the result.</p></div></div>

<div class="footdef"><sup><a id="fn.16" class="footnum" href="#fnr.16" role="doc-backlink">16</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">One might question here what kind of status the observer has from within quantum mechanics. Very rapidly, this should lead to questionning how systems are defined. You might end up being forced to consider the whole universe as the only physical system that makes sense. But then, what do you do with special relativity? What do you mean by the state of the universe when it is not accessible to you? These foundational questions will not be addressed here. Quantum mechanics will be taken from a purely operational view. Yet, it does not mean that Quantum Information Processing cannot be used to address them. One of the most celebrated example is the result \(MIP^* = RE\) from complexity theory that has implications into the structure of Hilbert spaces for infinite-dimensional composite systems.</p></div></div>

<div class="footdef"><sup><a id="fn.17" class="footnum" href="#fnr.17" role="doc-backlink">17</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">The additional reason for preferring a definition where states are explicitely observer-dependent is that it emphasizes the nature of states: they are the consequence of the observer's relation to a system, rather than representing a pre-existing property of the system itself without reference to an observer. This epistemic vs ontic view of quantum mechanics has been a heated debate since the early days of quantum mechanics. See for instance <a href="https://doi.org/10.1016/j.shpsb.2006.10.007">https://doi.org/10.1016/j.shpsb.2006.10.007</a>.</p></div></div>

<div class="footdef"><sup><a id="fn.18" class="footnum" href="#fnr.18" role="doc-backlink">18</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Spoiler: they are perfectly distinguishable as it would be expected for classical random variables. As a consequence, it is easy to embed a discrete classical probability distribution within a quantum state.</p></div></div>

<div class="footdef"><sup><a id="fn.19" class="footnum" href="#fnr.19" role="doc-backlink">19</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">This is in complete analogy with classical complexity theory, where bits a resources allowing access to exponentially many bit-string values.</p></div></div>

<div class="footdef"><sup><a id="fn.20" class="footnum" href="#fnr.20" role="doc-backlink">20</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Remember the state needs to be normalized.</p></div></div>

<div class="footdef"><sup><a id="fn.21" class="footnum" href="#fnr.21" role="doc-backlink">21</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Always pay attention to the definition of Pauli matrices when reading a research paper, as it might not be consistent throughout the whole paper! 3 normalization conventions co-exist: \(\tr I = 1\), \(\tr I = 2\) and \(\tr I = 2/\sqrt 2\).</p></div></div>

<div class="footdef"><sup><a id="fn.22" class="footnum" href="#fnr.22" role="doc-backlink">22</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">A basis of the form \(\ket {\psi_i} \otimes \ket{\psi'_j}\) to express the state.</p></div></div>

<div class="footdef"><sup><a id="fn.23" class="footnum" href="#fnr.23" role="doc-backlink">23</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">This same trick will be used to derive the Choi-Jamiolkowski isomorphism representation of Completely Positive Trace Preserving maps.</p></div></div>

 --></article>]]></content><author><name></name></author><category term="Lecture" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Introductory lecture on verification of quantum computations</title><link href="http://localhost:4000/2023/07/04/verification.html" rel="alternate" type="text/html" title="Introductory lecture on verification of quantum computations" /><published>2023-07-04T00:00:00+02:00</published><updated>2023-07-04T00:00:00+02:00</updated><id>http://localhost:4000/2023/07/04/verification</id><content type="html" xml:base="http://localhost:4000/2023/07/04/verification.html"><![CDATA[<p>This <a href="/assets/20230704.ucl.pdf">lecture</a> was given at UCL Advanced Summer School on Quantum Technologies July 4-5, 2023.</p>

<p>It introduces the basis ingredients of verified computations. It is framed using measurement based quantum computing as a convenient model to obtain blindness and design verification schemes. Comments and questions welcome. The last slides include a list of references.</p>]]></content><author><name></name></author><category term="Talk" /><summary type="html"><![CDATA[This lecture was given at UCL Advanced Summer School on Quantum Technologies July 4-5, 2023.]]></summary></entry><entry><title type="html">Highlights from “Limitations of variational quantum algorithms: a quantum optimal transport approach”</title><link href="http://localhost:4000/2023/04/17/concentration.html" rel="alternate" type="text/html" title="Highlights from “Limitations of variational quantum algorithms: a quantum optimal transport approach”" /><published>2023-04-17T00:00:00+02:00</published><updated>2023-04-17T00:00:00+02:00</updated><id>http://localhost:4000/2023/04/17/concentration</id><content type="html" xml:base="http://localhost:4000/2023/04/17/concentration.html"><![CDATA[<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "left",
        displayIndent: "5em",

        extensions: ["[Contrib]/physics/physics.js"],

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>

<div id="content" class="content">

Highlights from <a href="https://arxiv.org/abs/2204.03455">arXiv:2204.03455</a>.

<p>
\[
\newcommand{\one}{\mathbb{1}}
\newcommand{\Id}{\mathbb{I}}
\newcommand\supp{\mathrm{supp}}
\newcommand\tr{\mathrm{tr}}
\]
</p>

<div id="outline-container-orge810043" class="outline-2">
<h2 id="orge810043"><span class="section-number-2">1.</span> Definitions</h2>
<div class="outline-text-2" id="text-1">
<div class="definition" id="org1f2f8ba">
<p>
The Lipschitz constant for \(H\) a self-adjoint operator is defined by:
</p>
\begin{equation}
\|H\|_L = 2 \max_{v\in V} \min_{H_{\setminus v}} \|H - H_{\setminus v}\otimes \Id_v\|_{\infty}
\end{equation}

</div>

<div class="definition" id="orgd654d67">
<p>
The sandwiched Renyi divergence of order \(\alpha \in (1,+\infty)\) is defined for two quantum states \(\rho, \sigma\) with \(\supp \rho \subset \supp \sigma\) as
</p>
\begin{equation}
D_{\alpha} (\rho \| \sigma) = \frac{1}{\alpha-1} \log \tr \left[\left(\sigma^{\frac{1-\alpha}{2\alpha}} \rho \sigma^{\frac{1-\alpha}{2\alpha}}\right )^\alpha\right]
\end{equation}

</div>

<div class="definition" id="org3661770">
<p>
A state \(\sigma\) on \(V\) qudits satisfies a Gaussian concentration inequality of parameter \(c\) if there is a constant \(K\) such that for any \(a\) and any observable \(O\):
</p>
\begin{equation}
\Pr_{\sigma} (|O - \langle O\rangle_\sigma \Id| &gt; a |V|) \leq K \exp \left( - \frac{c a^2 |V|}{\| \sigma^{-1/2} O \sigma^{1/2}\|_L^2}\right)
\end{equation}

</div>
<p>
Above \(|A| = \sqrt {A^\dagger A}\), and \(\Pr_\sigma(|O-\langle
O\rangle_\sigma \Id &gt; a|V|)\) means that for \(E\) the positive part of
\(|O-\langle O\rangle_{\sigma}\Id|-a|V|\Id\), we have \(\tr(E\sigma)\)
bounded by the right hand side of the equation. Note that in the cases
where \(\sigma\) and \(O\) commute \(\| \sigma^{-1/2} O \sigma^{1/2}\|_L^2\)
reduces to \(\| O\|_L^2\).
</p>
</div>
</div>

<div id="outline-container-org283c48c" class="outline-2">
<h2 id="org283c48c"><span class="section-number-2">2.</span> Transferring inequalities</h2>
<div class="outline-text-2" id="text-2">
<div class="theorem" id="orgbdcac30">
<p>
Let \(\rho, \sigma\) be two quantum states on \(\mathcal H_V\). Then for any POVM element \(0 \leq E \leq \Id\) and \(\alpha &gt;1\),
</p>
\begin{equation}
\tr[E\rho] \leq \exp\left( \frac{\alpha -1}{\alpha} (D_\alpha(\rho\|\sigma) + \log(\tr[E\sigma]))\right)
\end{equation}

</div>

<div class="proof" id="org7e0e1fd">
<p>
Using the circularity of the trace we have:
</p>
\begin{align}
\tr[E\rho]
&amp; = \tr[\sigma^{-\frac{1-\alpha}{2\alpha}} E \sigma^{-\frac{1-\alpha}{2\alpha}} \times \sigma^{\frac{1-\alpha}{2\alpha}} \rho  \sigma^{\frac{1-\alpha}{2\alpha}}] \\
&amp; \leq \left\{\tr \left[ \left( \sigma^{-\frac{1-\alpha}{2\alpha}} E \sigma^{-\frac{1-\alpha}{2\alpha}}\right)^\beta \right ]\right\}^{\frac{1}{\beta}} \times \left\{\tr \left[ \left( \sigma^{\frac{1-\alpha}{2\alpha}} \rho \sigma^{\frac{1-\alpha}{2\alpha}} \right)^\alpha \right ]\right\}^{\frac{1}{\alpha}}
\end{align}
<p>
where the last line is obtained by applying H\"older inequality, which holds for \(\frac{1}{\alpha} + \frac{1}{\beta} = 1\).
</p>

<p>
Using the Araki-Lieb-Thirring inequality followed by \(E \leq \Id\) we obtain:
</p>
\begin{align}
\tr \left( \sigma^{-\frac{1-\alpha}{2\alpha}} E \sigma^{-\frac{1-\alpha}{2\alpha}}\right)^\beta
&amp; \leq \tr \left( \sigma^{-\frac{(1-\alpha)\beta}{2\alpha}} E^\beta \sigma^{-\frac{(1-\alpha)\beta}{2\alpha}}\right) \\
&amp; \leq \tr \left( \sigma^{-\frac{(1-\alpha)\beta}{2\alpha}} E \sigma^{-\frac{(1-\alpha)\beta}{2\alpha}}\right).
\end{align}
<p>
As \(\beta = \frac{\alpha}{\alpha -1}\) we have
</p>
\begin{equation}
\tr \left( \sigma^{-\frac{(1-\alpha)\beta}{2\alpha}} E \sigma^{-\frac{(1-\alpha)\beta}{2\alpha}}\right)=\tr(E\sigma).
\end{equation}

<p>
Additionally, note that
</p>
\begin{equation}
\left \{ \tr \left[ \left( \sigma^{\frac{1-\alpha}{2\alpha}} \rho  \sigma^{\frac{1-\alpha}{2\alpha}} \right)^\alpha \right ]\right\}^{\frac{1}{\alpha}} = \exp (\frac{\alpha -1}{\alpha} D_\alpha(\rho\|\sigma)).
\end{equation}

<p>
Combining the two gives:
</p>
\begin{align}
\tr[E\rho]
&amp; \leq (tr[E\sigma]) \times \exp\left( \frac{\alpha -1}{\alpha} (D_\alpha(\rho\|\sigma))\right) \\
&amp; \leq \exp\left( \frac{\alpha -1}{\alpha} (D_\alpha(\rho\|\sigma))  \log (\tr[E\sigma])\right).
\end{align}

</div>

<p>
Remark that the theorem bounds the probability of observing \(E\) on
\(\rho\) based on the product of the probability of observing \(E\) on
\(\sigma\) times a quantity that depends on \(D_\alpha(\rho\|\sigma)\),
which can be seen as a measure of the distance between the two states
\(\rho\) and \(\sigma\). This will become interesting whenever
\(D_\alpha(\rho\|\sigma)\) becomes small.
</p>

<div class="corollary" id="org4695ae0">
<p>
If \(\sigma\) satisfies a Gaussian concentration inequality 
</p>
\begin{equation}
\Pr_{\sigma} (|O - \langle O\rangle_\sigma \Id| &gt; a |V|) \leq K \exp \left( - \frac{c a^2 |V|}{\| \sigma^{-1/2} O \sigma^{1/2}\|_L^2}\right)
\end{equation}
<p>
for some constant \(c, K &gt; 0\), then for any \(\alpha &gt; 1\): 
</p>
\begin{equation}
\Pr_{\rho} (|O - \langle O\rangle_\sigma \Id| &gt; a |V|) \leq \exp  \left(\frac{\alpha -1}{\alpha}\left(D_\alpha(\rho\|\sigma) - \frac{c a^2 |V|}{\| \sigma^{-1/2} O \sigma^{1/2}\|_L^2} + \log(K)\right)\right)
\end{equation}

</div>

<div class="corollary" id="org57fda40">
<p>
For an input state \(\rho\), a noisy circuit evolution \(\mathcal{N}(\rho)\) and a state \(\sigma\) satisfying a Gaussian concentration inequality, whenever there is a value of \(\alpha\) and \(a\) such that
</p>
\begin{equation}
\frac{D_{\alpha}(\mathcal N(\rho) \| \sigma)}{|V|} &lt; \frac{c a^2}{\| \sigma^{-1/2} O \sigma^{1/2}\|_L^2} - \frac{\log(K)}{|V|},
\end{equation}
<p>
then the probability of observing an outcome of \(O\) outside the interval \(\langle O \rangle_{\sigma} \pm a|V|\) decreases exponentially with \(|V|\).
</p>

</div>
</div>
</div>

<div id="outline-container-orgcb0f66f" class="outline-2">
<h2 id="orgcb0f66f"><span class="section-number-2">3.</span> Bounding \(D_{\alpha}(\mathcal N(\rho) \| \sigma)\)</h2>
<div class="outline-text-2" id="text-3">
<p>
Such bounds can be obtained for noisy circuits where each layer \(i\) is followed by a noisy channel \(\mathcal N\) with a fixed point \(\sigma\) satisfying a strong data processing inequality:
</p>
\begin{equation}
D_\alpha(\mathcal N(\rho) \| \sigma)\leq (1-q) D_\alpha (\rho \|\sigma), \ \forall \rho.
\end{equation}

<p>
In such cases we obtain
</p>
<div class="lemma" id="org7432983">
<p>
Let \(\mathcal N\) be a quantum channel with a unique fixed point \(\sigma\) and satisfying a strong data processing inequality for some \(\alpha &gt; 1\), then for any other channels \(\{\Phi_i\}_{i\leq l}\) we have:
</p>
\begin{equation}
D_\alpha\left( \mathcal P (\rho) \| \sigma \right) \leq (1-q)^l D_\alpha(\rho\|\sigma) + \sum_{i\leq l} (1-q)^{l-i} D_\infty (\Phi_i(\sigma) \| \sigma)
\end{equation}
<p>
where \(D_\infty(\rho \| \sigma) = \log \| \sigma^{-\frac{1}{2}} \rho \sigma^{-\frac{1}{2}}\|_\infty\), and \(\mathcal P = \prod_{i \leq l} \Phi_i \circ \mathcal N\).
</p>

</div>

<div class="proof" id="org4062ecd">
<p>
We proceed by induction on \(l\). For \(l=1\), we use the data processed triangle inequality:
</p>
\begin{equation}
D_\alpha(\Phi_1 \circ \mathcal N  (\rho) \|\sigma) \leq D_\alpha(\mathcal N (\rho) \| \sigma) + D_\infty (\Phi_1 (\sigma) \| \sigma) \leq (1-q) D_\alpha(\rho \| \sigma) + D_\infty (\Phi_1 (\sigma) \| \sigma).
\end{equation}
<p>
The induction is performed in the same way, assuming the property holds for some \(l\) and then applying the data processed triangle inequality.
</p>

</div>
<p>
Note that for unital channels \(\mathcal N(\sigma) = \sigma\) as the
fixed point of the channel is \(\Id\). This implies that \(D_\alpha\left(
\mathcal P (\rho) \| \sigma \right)\) will always converge to 0.
</p>
</div>
</div>

<div id="outline-container-org637fc60" class="outline-2">
<h2 id="org637fc60"><span class="section-number-2">4.</span> Additional notes</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org9373a48" class="outline-3">
<h3 id="org9373a48"><span class="section-number-3">4.1.</span> Renyi entropies</h3>
<div class="outline-text-3" id="text-4-1">
<p>
In the above definition of Renyi divergence, note that when \(\alpha \rightarrow 1\) we recover the usual relative entropy: 
</p>
\begin{equation}
D_1 (\rho \| \sigma) =  \tr(\rho (\log \rho - \log \sigma)) 
\end{equation}

<p>
This is not a surprise as these sandwiched Renyi divergences have been introduced as a generalization of the usual relative entropy. Renyi entropies \(\frac{1}{1-\alpha} \log \frac{\tr \rho^\alpha}{\tr \rho}\) are also generalizations of von Neumann entropy in that they preserve important properties:
</p>
<ul class="org-ul">
<li>Continuity</li>
<li>Unitary invariance</li>
<li>Normalization</li>
<li>Additivity.</li>
</ul>
</div>
</div>

<div id="outline-container-org1f276fc" class="outline-3">
<h3 id="org1f276fc"><span class="section-number-3">4.2.</span> Relative entropy and Pinsker inequality</h3>
<div class="outline-text-3" id="text-4-2">
<p>
The Pinsker inequality relates the relative entropy to the trace distance. As such it allows to bound the trace distance using information theoretic arguments:
</p>
\begin{equation}
D_1(\rho\|\sigma) \geq \frac{1}{2\log 2} \| \rho -\sigma \|_{\tr}^2
\end{equation}
</div>
</div>
<div id="outline-container-orgc5c639a" class="outline-3">
<h3 id="orgc5c639a"><span class="section-number-3">4.3.</span> Hölder inequality</h3>
<div class="outline-text-3" id="text-4-3">
<p>
For \(A\), \(B\) operators on \(\mathcal H\)
</p>
\begin{equation}
\|AB\|_1 \leq \|A\|_p \|B\|_q, \quad \frac{1}{p} + \frac{1}{q} = 1
\end{equation}
<p>
which translates for matrices into
</p>
\begin{equation}
\tr(|AB|) \leq [\tr(|A|^{p})]^{1/p} \times [\tr(|B|^{q})]^{1/q}, \quad \frac{1}{p} + \frac{1}{q} = 1.
\end{equation}
</div>
</div>
<div id="outline-container-orgc11c70e" class="outline-3">
<h3 id="orgc11c70e"><span class="section-number-3">4.4.</span> Araki-Lieb-Thirring inequality</h3>
<div class="outline-text-3" id="text-4-4">
<p>
For \(A,B \geq 0\), \(q\geq 0\) and \(0\leq r \leq 1\) then
</p>
\begin{equation}
\tr[(A^rB^rA^r)^q] \leq\tr[(ABA)^{rq}].
\end{equation}
<p>
When \(r \geq 1\) the inequality is reversed.
</p>
</div>
</div>
</div>
</div>]]></content><author><name></name></author><category term="Qatalyze" /><summary type="html"><![CDATA[]]></summary></entry></feed>